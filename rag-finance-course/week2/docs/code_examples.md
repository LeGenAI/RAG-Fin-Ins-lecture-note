# 2ì£¼ì°¨ ì½”ë“œ ì˜ˆì œ ëª¨ìŒ

## ëª©ì°¨
1. [ë°ì´í„° ë³€í™˜ ì˜ˆì œ](#1-ë°ì´í„°-ë³€í™˜-ì˜ˆì œ)
2. [ë²¡í„° DB êµ¬ì¶• ì˜ˆì œ](#2-ë²¡í„°-db-êµ¬ì¶•-ì˜ˆì œ)
3. [RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì œ](#3-rag-ì‹œìŠ¤í…œ-êµ¬í˜„-ì˜ˆì œ)
4. [í†µí•© íŒŒì´í”„ë¼ì¸ ì˜ˆì œ](#4-í†µí•©-íŒŒì´í”„ë¼ì¸-ì˜ˆì œ)

---

## 1. ë°ì´í„° ë³€í™˜ ì˜ˆì œ

### 1.1 CSV to JSON ë³€í™˜ê¸°

```python
import pandas as pd
import json
from datetime import datetime
import re

class BankStatementConverter:
    """ì€í–‰ ê±°ë˜ë‚´ì—­ ë³€í™˜ê¸°"""
    
    def __init__(self, bank_name='KBêµ­ë¯¼ì€í–‰'):
        self.bank_name = bank_name
        self.encoding_map = {
            'KBêµ­ë¯¼ì€í–‰': 'cp949',
            'ì‹ í•œì€í–‰': 'utf-8',
            'ìš°ë¦¬ì€í–‰': 'euc-kr',
            'í•˜ë‚˜ì€í–‰': 'cp949',
            'ì¹´ì¹´ì˜¤ë±…í¬': 'utf-8'
        }
        
    def csv_to_json(self, csv_file_path, output_path=None):
        """CSVë¥¼ JSONìœ¼ë¡œ ë³€í™˜"""
        
        # 1. CSV íŒŒì¼ ë¡œë“œ
        encoding = self.encoding_map.get(self.bank_name, 'utf-8')
        df = pd.read_csv(csv_file_path, encoding=encoding)
        
        # 2. ì»¬ëŸ¼ëª… í‘œì¤€í™”
        df = self._standardize_columns(df)
        
        # 3. ë°ì´í„° ì •ì œ
        df = self._clean_data(df)
        
        # 4. AIìš© ë©”íƒ€ë°ì´í„° ì¶”ê°€
        df = self._enrich_with_ai_metadata(df)
        
        # 5. JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        result = self._create_json_structure(df)
        
        # 6. íŒŒì¼ ì €ì¥
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        return result
    
    def _standardize_columns(self, df):
        """ì»¬ëŸ¼ëª… í‘œì¤€í™”"""
        column_mapping = {
            'ê±°ë˜ì¼ì': 'date',
            'ê±°ë˜ì¼': 'date',
            'ê±°ë˜ì‹œê°„': 'time',
            'ì ìš”': 'description',
            'ê±°ë˜ë‚´ìš©': 'description',
            'ì¶œê¸ˆì•¡': 'withdrawal',
            'ì…ê¸ˆì•¡': 'deposit',
            'ì”ì•¡': 'balance',
            'ê±°ë˜ì ': 'channel'
        }
        return df.rename(columns=column_mapping)
    
    def _clean_data(self, df):
        """ë°ì´í„° ì •ì œ"""
        # ê¸ˆì•¡ í•„ë“œ ìˆ«ìë¡œ ë³€í™˜
        for col in ['withdrawal', 'deposit', 'balance']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
        
        return df
    
    def _enrich_with_ai_metadata(self, df):
        """AI ë¶„ì„ìš© ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        
        # ê±°ë˜ ìœ í˜• ë¶„ë¥˜
        df['transaction_type'] = df.apply(
            lambda x: 'withdrawal' if x.get('withdrawal', 0) > 0 else 'deposit',
            axis=1
        )
        
        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
        df['category'] = df['description'].apply(self._categorize_transaction)
        
        # ì‹œê°„ëŒ€ ë¶„ì„
        if 'time' in df.columns:
            df['time_period'] = df['time'].apply(self._get_time_period)
        
        # ìš”ì¼ ì •ë³´
        if 'date' in df.columns:
            df['weekday'] = df['date'].dt.day_name()
            df['is_weekend'] = df['date'].dt.weekday >= 5
        
        return df
    
    def _categorize_transaction(self, description):
        """ê±°ë˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        categories = {
            'ì‹ìŒë£Œ': ['ì¹´í˜', 'ì»¤í”¼', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì‹ë‹¹', 'ë§¥ë„ë‚ ë“œ', 'ë°°ë‹¬'],
            'êµí†µ': ['ì§€í•˜ì² ', 'ë²„ìŠ¤', 'íƒì‹œ', 'ì£¼ìœ ', 'í†¨ê²Œì´íŠ¸'],
            'ì‡¼í•‘': ['ë§ˆíŠ¸', 'ì´ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì˜¨ë¼ì¸', 'ì¿ íŒ¡'],
            'ì˜ë£Œ': ['ë³‘ì›', 'ì•½êµ­', 'ì˜ì›'],
            'ê¸ˆìœµ': ['ì´ì²´', 'ëŒ€ì¶œ', 'ì´ì', 'ATM'],
            'í†µì‹ ': ['KT', 'SKT', 'LG', 'í†µì‹ '],
            'ê¸‰ì—¬': ['ê¸‰ì—¬', 'ì›”ê¸‰', 'ìƒì—¬'],
            'ê³µê³¼ê¸ˆ': ['ì „ê¸°', 'ê°€ìŠ¤', 'ìˆ˜ë„', 'ê´€ë¦¬ë¹„']
        }
        
        description_lower = description.lower() if description else ''
        
        for category, keywords in categories.items():
            if any(keyword.lower() in description_lower for keyword in keywords):
                return category
        
        return 'ê¸°íƒ€'
    
    def _get_time_period(self, time_str):
        """ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        try:
            hour = int(time_str.split(':')[0])
            if 6 <= hour < 9:
                return 'ì•„ì¹¨'
            elif 9 <= hour < 12:
                return 'ì˜¤ì „'
            elif 12 <= hour < 14:
                return 'ì ì‹¬'
            elif 14 <= hour < 18:
                return 'ì˜¤í›„'
            elif 18 <= hour < 21:
                return 'ì €ë…'
            else:
                return 'ë°¤'
        except:
            return 'ê¸°íƒ€'
    
    def _create_json_structure(self, df):
        """ìµœì¢… JSON êµ¬ì¡° ìƒì„±"""
        
        transactions = []
        for _, row in df.iterrows():
            transaction = {
                'id': f"{self.bank_name}_{row.name}_{datetime.now().timestamp()}",
                'date': row['date'].isoformat() if pd.notna(row['date']) else None,
                'time': row.get('time', '00:00:00'),
                'description': row['description'],
                'amount': float(row['withdrawal']) if row['transaction_type'] == 'withdrawal' else float(row['deposit']),
                'type': row['transaction_type'],
                'balance': float(row['balance']),
                'category': row['category'],
                'channel': row.get('channel', 'unknown'),
                'metadata': {
                    'weekday': row.get('weekday'),
                    'is_weekend': row.get('is_weekend'),
                    'time_period': row.get('time_period'),
                    'bank': self.bank_name
                }
            }
            transactions.append(transaction)
        
        return {
            'version': '1.0',
            'generated_at': datetime.now().isoformat(),
            'bank': self.bank_name,
            'summary': {
                'total_transactions': len(transactions),
                'date_range': {
                    'start': df['date'].min().isoformat() if not df.empty else None,
                    'end': df['date'].max().isoformat() if not df.empty else None
                },
                'total_deposits': float(df[df['transaction_type'] == 'deposit']['deposit'].sum()),
                'total_withdrawals': float(df[df['transaction_type'] == 'withdrawal']['withdrawal'].sum()),
                'category_breakdown': df.groupby('category')['withdrawal'].sum().to_dict()
            },
            'transactions': transactions
        }

# ì‚¬ìš© ì˜ˆì‹œ
converter = BankStatementConverter('KBêµ­ë¯¼ì€í–‰')
json_data = converter.csv_to_json('kb_statement.csv', 'kb_statement.json')
print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(json_data['transactions'])}ê±´ì˜ ê±°ë˜")
```

### 1.2 ë³´í—˜ ì•½ê´€ êµ¬ì¡°í™” ë³€í™˜ê¸°

```python
import re
from typing import Dict, List
import json

class InsurancePolicyParser:
    """ë³´í—˜ ì•½ê´€ íŒŒì„œ"""
    
    def __init__(self):
        self.parsed_clauses = []
        
    def parse_policy_text(self, policy_text: str) -> Dict:
        """ë³´í—˜ ì•½ê´€ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
        
        # ì¡°í•­ ì¶”ì¶œ
        clauses = self._extract_clauses(policy_text)
        
        # ê° ì¡°í•­ íŒŒì‹±
        parsed_clauses = []
        for clause in clauses:
            parsed = self._parse_clause(clause)
            if parsed:
                parsed_clauses.append(parsed)
        
        # ë³´ì¥ ë‚´ìš© ì¶”ì¶œ
        coverages = self._extract_coverages(policy_text)
        
        # ë©´ì±… ì‚¬í•­ ì¶”ì¶œ
        exclusions = self._extract_exclusions(policy_text)
        
        return {
            'type': 'insurance_policy',
            'clauses': parsed_clauses,
            'coverages': coverages,
            'exclusions': exclusions,
            'metadata': {
                'total_clauses': len(parsed_clauses),
                'total_coverages': len(coverages),
                'total_exclusions': len(exclusions)
            }
        }
    
    def _extract_clauses(self, text: str) -> List[str]:
        """ì¡°í•­ ì¶”ì¶œ"""
        # ì •ê·œì‹ìœ¼ë¡œ 'ì œXì¡°' íŒ¨í„´ ì°¾ê¸°
        pattern = r'ì œ\d+ì¡°[^ì œ]*'
        clauses = re.findall(pattern, text)
        return clauses
    
    def _parse_clause(self, clause_text: str) -> Dict:
        """ê°œë³„ ì¡°í•­ íŒŒì‹±"""
        # ì¡°í•­ ë²ˆí˜¸ ì¶”ì¶œ
        article_match = re.match(r'ì œ(\d+)ì¡°', clause_text)
        if not article_match:
            return None
        
        article_number = int(article_match.group(1))
        
        # ì¡°í•­ ì œëª© ì¶”ì¶œ
        title_match = re.search(r'ì œ\d+ì¡°\s*\(([^)]+)\)', clause_text)
        title = title_match.group(1) if title_match else ''
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = re.sub(r'ì œ\d+ì¡°\s*\([^)]*\)', '', clause_text).strip()
        
        # í•˜ìœ„ í•­ëª© ì¶”ì¶œ
        subitems = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*([^â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]*)', content)
        
        return {
            'article_number': article_number,
            'title': title,
            'content': content,
            'subitems': subitems,
            'keywords': self._extract_keywords(content)
        }
    
    def _extract_coverages(self, text: str) -> List[Dict]:
        """ë³´ì¥ ë‚´ìš© ì¶”ì¶œ"""
        coverages = []
        
        # ë³´ì¥ ê´€ë ¨ í‚¤ì›Œë“œ
        coverage_keywords = ['ë³´ì¥', 'ì§€ê¸‰', 'ë³´ìƒ', 'ê¸‰ì—¬', 'í˜œíƒ']
        
        lines = text.split('\n')
        for line in lines:
            if any(keyword in line for keyword in coverage_keywords):
                # ê¸ˆì•¡ ì¶”ì¶œ
                amount_match = re.search(r'(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*ì›', line)
                amount = amount_match.group(1) if amount_match else None
                
                if amount:
                    coverages.append({
                        'description': line.strip(),
                        'amount': amount,
                        'type': self._classify_coverage_type(line)
                    })
        
        return coverages
    
    def _extract_exclusions(self, text: str) -> List[str]:
        """ë©´ì±… ì‚¬í•­ ì¶”ì¶œ"""
        exclusions = []
        
        # ë©´ì±… ê´€ë ¨ í‚¤ì›Œë“œ
        exclusion_keywords = ['ë©´ì±…', 'ì œì™¸', 'ë³´ìƒí•˜ì§€ ì•Š', 'ì§€ê¸‰í•˜ì§€ ì•Š']
        
        lines = text.split('\n')
        for line in lines:
            if any(keyword in line for keyword in exclusion_keywords):
                exclusions.append(line.strip())
        
        return exclusions
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì¤‘ìš” ê¸ˆìœµ/ë³´í—˜ ìš©ì–´
        important_terms = [
            'ë³´í—˜ê¸ˆ', 'ë³´í—˜ë£Œ', 'í”¼ë³´í—˜ì', 'ìˆ˜ìµì', 'ê³„ì•½ì',
            'ë³´ì¥', 'ë©´ì±…', 'í•´ì§€', 'ë§Œê¸°', 'ê°±ì‹ ',
            'ì§„ë‹¨', 'ì…ì›', 'ìˆ˜ìˆ ', 'í†µì›', 'ì‚¬ë§'
        ]
        
        found_keywords = []
        for term in important_terms:
            if term in text:
                found_keywords.append(term)
        
        return found_keywords
    
    def _classify_coverage_type(self, text: str) -> str:
        """ë³´ì¥ ìœ í˜• ë¶„ë¥˜"""
        if 'ì‚¬ë§' in text:
            return 'ì‚¬ë§ë³´ì¥'
        elif 'ì…ì›' in text:
            return 'ì…ì›ë³´ì¥'
        elif 'ìˆ˜ìˆ ' in text:
            return 'ìˆ˜ìˆ ë³´ì¥'
        elif 'ì§„ë‹¨' in text:
            return 'ì§„ë‹¨ë³´ì¥'
        elif 'ë§Œê¸°' in text:
            return 'ë§Œê¸°ë³´ì¥'
        else:
            return 'ê¸°íƒ€ë³´ì¥'
    
    def save_as_json(self, parsed_data: Dict, output_path: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(parsed_data, f, ensure_ascii=False, indent=2)

# ì‚¬ìš© ì˜ˆì‹œ
parser = InsurancePolicyParser()

sample_policy = """
ì œ1ì¡°(ëª©ì ) ì´ ë³´í—˜ê³„ì•½ì€ ë³´í—˜ê³„ì•½ìê°€ ë‚©ì…í•œ ë³´í—˜ë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ í”¼ë³´í—˜ìê°€ ë³´í—˜ê¸°ê°„ ì¤‘ 
ë³´í—˜ê¸ˆ ì§€ê¸‰ì‚¬ìœ ê°€ ë°œìƒí•  ê²½ìš° ë³´í—˜ìˆ˜ìµìì—ê²Œ ì•½ì •í•œ ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•¨ì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.

ì œ2ì¡°(ë³´í—˜ê¸ˆì˜ ì§€ê¸‰)
â‘  íšŒì‚¬ëŠ” í”¼ë³´í—˜ìê°€ ì‚¬ë§í•œ ê²½ìš° 1ì–µì›ì„ ì§€ê¸‰í•©ë‹ˆë‹¤.
â‘¡ íšŒì‚¬ëŠ” í”¼ë³´í—˜ìê°€ ì•”ìœ¼ë¡œ ì§„ë‹¨ë°›ì€ ê²½ìš° 3ì²œë§Œì›ì„ ì§€ê¸‰í•©ë‹ˆë‹¤.

ì œ3ì¡°(ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•˜ì§€ ì•ŠëŠ” ì‚¬ìœ )
íšŒì‚¬ëŠ” ë‹¤ìŒì˜ ê²½ìš° ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
1. ê³ ì˜ì  ì‚¬ê³ 
2. ì „ìŸ, í˜ëª…
"""

parsed = parser.parse_policy_text(sample_policy)
parser.save_as_json(parsed, 'parsed_policy.json')
print("âœ… ë³´í—˜ ì•½ê´€ íŒŒì‹± ì™„ë£Œ")
```

### 1.3 Markdown ë³´ê³ ì„œ ìƒì„±ê¸°

```python
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import base64
from io import BytesIO

class FinancialReportGenerator:
    """ê¸ˆìœµ ê±°ë˜ Markdown ë³´ê³ ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, transaction_df: pd.DataFrame):
        self.df = transaction_df
        self.report_sections = []
        
    def generate_full_report(self) -> str:
        """ì „ì²´ ë³´ê³ ì„œ ìƒì„±"""
        
        # í—¤ë”
        self._add_header()
        
        # ìš”ì•½ í†µê³„
        self._add_summary_statistics()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        self._add_category_analysis()
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        self._add_temporal_analysis()
        
        # AI ì¸ì‚¬ì´íŠ¸
        self._add_ai_insights()
        
        # ì¶”ì²œì‚¬í•­
        self._add_recommendations()
        
        return '\n\n'.join(self.report_sections)
    
    def _add_header(self):
        """ë³´ê³ ì„œ í—¤ë” ì¶”ê°€"""
        header = f"""# ğŸ“Š ê¸ˆìœµ ê±°ë˜ ë¶„ì„ ë³´ê³ ì„œ

**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}  
**ë¶„ì„ê¸°ê°„**: {self.df['date'].min()} ~ {self.df['date'].max()}  
**ì´ ê±°ë˜ê±´ìˆ˜**: {len(self.df):,}ê±´"""
        
        self.report_sections.append(header)
    
    def _add_summary_statistics(self):
        """ìš”ì•½ í†µê³„ ì¶”ê°€"""
        total_income = self.df[self.df['type'] == 'deposit']['amount'].sum()
        total_expense = self.df[self.df['type'] == 'withdrawal']['amount'].sum()
        net_flow = total_income - total_expense
        
        summary = f"""## ğŸ’° ì¬ë¬´ ìš”ì•½

| í•­ëª© | ê¸ˆì•¡ |
|------|------|
| **ì´ ìˆ˜ì…** | {total_income:,.0f}ì› |
| **ì´ ì§€ì¶œ** | {total_expense:,.0f}ì› |
| **ìˆœ í˜„ê¸ˆíë¦„** | {net_flow:+,.0f}ì› |
| **í‰ê·  ì¼ì¼ ì§€ì¶œ** | {total_expense / 30:,.0f}ì› |
| **ìµœëŒ€ ë‹¨ì¼ ì§€ì¶œ** | {self.df[self.df['type'] == 'withdrawal']['amount'].max():,.0f}ì› |"""
        
        self.report_sections.append(summary)
    
    def _add_category_analysis(self):
        """ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì¶”ê°€"""
        category_spending = self.df[self.df['type'] == 'withdrawal'].groupby('category')['amount'].agg(['sum', 'count', 'mean'])
        category_spending = category_spending.sort_values('sum', ascending=False)
        
        category_section = "## ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ ë¶„ì„\n\n"
        
        # ì°¨íŠ¸ ìƒì„±
        chart = self._create_pie_chart(category_spending['sum'])
        category_section += f"![ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ]({chart})\n\n"
        
        # í…Œì´ë¸”
        category_section += "### ìƒì„¸ ë‚´ì—­\n\n"
        category_section += "| ì¹´í…Œê³ ë¦¬ | ì´ì•¡ | ê±´ìˆ˜ | í‰ê·  | ë¹„ì¤‘ |\n"
        category_section += "|---------|------|------|------|------|\n"
        
        total_spending = category_spending['sum'].sum()
        for category, row in category_spending.iterrows():
            percentage = (row['sum'] / total_spending) * 100
            category_section += f"| {category} | {row['sum']:,.0f}ì› | {row['count']:.0f}ê±´ | {row['mean']:,.0f}ì› | {percentage:.1f}% |\n"
        
        self.report_sections.append(category_section)
    
    def _add_temporal_analysis(self):
        """ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì¶”ê°€"""
        # ìš”ì¼ë³„ ë¶„ì„
        weekday_spending = self.df[self.df['type'] == 'withdrawal'].groupby('weekday')['amount'].sum()
        
        temporal = "## ğŸ“… ì‹œê°„ëŒ€ë³„ ë¶„ì„\n\n"
        temporal += "### ìš”ì¼ë³„ ì§€ì¶œ íŒ¨í„´\n\n"
        
        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        temporal += "```\n"
        for day in weekday_order:
            if day in weekday_spending:
                amount = weekday_spending[day]
                bar_length = int(amount / weekday_spending.max() * 30)
                temporal += f"{day[:3]:>3} | {'â–ˆ' * bar_length} {amount:,.0f}ì›\n"
        temporal += "```"
        
        self.report_sections.append(temporal)
    
    def _add_ai_insights(self):
        """AI ì¸ì‚¬ì´íŠ¸ ì¶”ê°€"""
        insights = "## ğŸ¤– AI ë¶„ì„ ì¸ì‚¬ì´íŠ¸\n\n"
        
        # ì§€ì¶œ íŒ¨í„´ ë¶„ì„
        avg_daily_spending = self.df[self.df['type'] == 'withdrawal']['amount'].sum() / 30
        high_spending_days = self.df[self.df['type'] == 'withdrawal'].groupby('date')['amount'].sum()
        high_spending_days = high_spending_days[high_spending_days > avg_daily_spending * 2]
        
        insights += "### ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n"
        insights += f"1. **ì§€ì¶œ íŒ¨í„´**: ì¼í‰ê·  {avg_daily_spending:,.0f}ì› ì§€ì¶œ\n"
        insights += f"2. **ê³ ì•¡ ì§€ì¶œì¼**: {len(high_spending_days)}ì¼ (í‰ê· ì˜ 2ë°° ì´ˆê³¼)\n"
        
        # ì ˆì•½ ê°€ëŠ¥ í•­ëª©
        top_category = self.df[self.df['type'] == 'withdrawal'].groupby('category')['amount'].sum().idxmax()
        top_amount = self.df[self.df['type'] == 'withdrawal'].groupby('category')['amount'].sum().max()
        
        insights += f"3. **ìµœëŒ€ ì§€ì¶œ ì¹´í…Œê³ ë¦¬**: {top_category} ({top_amount:,.0f}ì›)\n"
        insights += f"4. **ì ˆì•½ ì ì¬ë ¥**: {top_category}ì—ì„œ 20% ì ˆê° ì‹œ ì›” {top_amount * 0.2:,.0f}ì› ì ˆì•½ ê°€ëŠ¥\n"
        
        self.report_sections.append(insights)
    
    def _add_recommendations(self):
        """ì¶”ì²œì‚¬í•­ ì¶”ê°€"""
        recommendations = "## ğŸ’¡ ë§ì¶¤í˜• ì¶”ì²œì‚¬í•­\n\n"
        
        recommendations += """### ì§€ì¶œ ê´€ë¦¬
- [ ] ê³ ì • ì§€ì¶œ ê²€í†  ë° ìµœì í™”
- [ ] ì¶©ë™êµ¬ë§¤ ë°©ì§€ë¥¼ ìœ„í•œ 24ì‹œê°„ ê·œì¹™ ì ìš©
- [ ] ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì‚° ì„¤ì •

### ì €ì¶• ë° íˆ¬ì
- [ ] ì›” ìˆ˜ì…ì˜ 20% ì´ìƒ ì €ì¶• ëª©í‘œ ì„¤ì •
- [ ] ë¹„ìƒê¸ˆ 3-6ê°œì›”ì¹˜ ìƒí™œë¹„ í™•ë³´
- [ ] ì¥ê¸° íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ê²€í† 

### ê¸ˆìœµ ìƒí’ˆ í™œìš©
- [ ] ê³ ê¸ˆë¦¬ ì ê¸ˆ ìƒí’ˆ ê°€ì… ê²€í† 
- [ ] ì‹ ìš©ì¹´ë“œ í˜œíƒ ìµœì í™”
- [ ] ì„¸ê¸ˆ ì ˆê° ìƒí’ˆ í™œìš©"""
        
        self.report_sections.append(recommendations)
    
    def _create_pie_chart(self, data):
        """íŒŒì´ ì°¨íŠ¸ ìƒì„± (Base64 ì¸ì½”ë”©)"""
        plt.figure(figsize=(8, 6))
        plt.pie(data.values, labels=data.index, autopct='%1.1f%%')
        plt.title('ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ ë¹„ì¤‘')
        
        # Base64ë¡œ ì¸ì½”ë”©
        buffer = BytesIO()
        plt.savefig(buffer, format='png')
        buffer.seek(0)
        image_base64 = base64.b64encode(buffer.getvalue()).decode()
        plt.close()
        
        return f"data:image/png;base64,{image_base64}"
    
    def save_report(self, output_path: str):
        """ë³´ê³ ì„œ íŒŒì¼ë¡œ ì €ì¥"""
        report = self.generate_full_report()
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)

# ì‚¬ìš© ì˜ˆì‹œ
# DataFrame ì¤€ë¹„ (ì‹¤ì œ ë°ì´í„° í•„ìš”)
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=100),
    'amount': np.random.randint(1000, 100000, 100),
    'type': np.random.choice(['deposit', 'withdrawal'], 100, p=[0.2, 0.8]),
    'category': np.random.choice(['ì‹ìŒë£Œ', 'ì‡¼í•‘', 'êµí†µ', 'ì˜ë£Œ'], 100),
    'weekday': pd.date_range('2024-01-01', periods=100).day_name()
})

generator = FinancialReportGenerator(df)
generator.save_report('financial_report.md')
print("âœ… Markdown ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
```

---

## 2. ë²¡í„° DB êµ¬ì¶• ì˜ˆì œ

### 2.1 ChromaDB ê¸ˆìœµ ë°ì´í„° ê´€ë¦¬

```python
import chromadb
from chromadb.utils import embedding_functions
import json
from typing import List, Dict, Optional
import hashlib
from datetime import datetime

class FinancialChromaDB:
    """ê¸ˆìœµ ë°ì´í„° ì „ìš© ChromaDB ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, persist_directory: str = "./chroma_financial_db"):
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • (OpenAI ì‚¬ìš©)
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key="your-openai-api-key",
            model_name="text-embedding-ada-002"
        )
        
        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        self._init_collections()
    
    def _init_collections(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        self.collections = {}
        
        # ê±°ë˜ ë°ì´í„° ì»¬ë ‰ì…˜
        self.collections['transactions'] = self.client.get_or_create_collection(
            name="transactions",
            embedding_function=self.embedding_function,
            metadata={"description": "Financial transactions"}
        )
        
        # ê³ ê° í”„ë¡œíŒŒì¼ ì»¬ë ‰ì…˜
        self.collections['profiles'] = self.client.get_or_create_collection(
            name="customer_profiles",
            embedding_function=self.embedding_function,
            metadata={"description": "Customer profiles"}
        )
        
        # ê¸ˆìœµ ìƒí’ˆ ì»¬ë ‰ì…˜
        self.collections['products'] = self.client.get_or_create_collection(
            name="financial_products",
            embedding_function=self.embedding_function,
            metadata={"description": "Financial products"}
        )
    
    def add_transactions_batch(self, transactions: List[Dict], user_id: str):
        """ê±°ë˜ ë°ì´í„° ì¼ê´„ ì¶”ê°€"""
        documents = []
        metadatas = []
        ids = []
        
        for trans in transactions:
            # ë¬¸ì„œ ìƒì„± (ìì—°ì–´ í˜•íƒœ)
            doc = self._create_transaction_document(trans)
            documents.append(doc)
            
            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'user_id': user_id,
                'date': trans['date'],
                'amount': float(trans['amount']),
                'category': trans.get('category', 'unknown'),
                'type': trans['type'],
                'timestamp': datetime.now().isoformat()
            }
            metadatas.append(metadata)
            
            # ê³ ìœ  ID ìƒì„±
            id_string = f"{user_id}_{trans['date']}_{trans['amount']}_{len(ids)}"
            unique_id = hashlib.sha256(id_string.encode()).hexdigest()[:16]
            ids.append(unique_id)
        
        # ChromaDBì— ì¶”ê°€
        self.collections['transactions'].add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        return len(documents)
    
    def _create_transaction_document(self, transaction: Dict) -> str:
        """ê±°ë˜ë¥¼ ìì—°ì–´ ë¬¸ì„œë¡œ ë³€í™˜"""
        template = """
        ê±°ë˜ ì •ë³´:
        ë‚ ì§œ: {date}
        ì‹œê°„: {time}
        ê±°ë˜ìœ í˜•: {type}
        ê¸ˆì•¡: {amount:,}ì›
        ìƒí˜¸/ì„¤ëª…: {description}
        ì¹´í…Œê³ ë¦¬: {category}
        ì”ì•¡: {balance:,}ì›
        
        ê±°ë˜ ë§¥ë½:
        ì´ ê±°ë˜ëŠ” {category} ì¹´í…Œê³ ë¦¬ì˜ {type} ê±°ë˜ì…ë‹ˆë‹¤.
        {amount:,}ì›ì´ {action}ë˜ì—ˆìœ¼ë©°, ê±°ë˜ í›„ ì”ì•¡ì€ {balance:,}ì›ì…ë‹ˆë‹¤.
        """
        
        action = "ì¶œê¸ˆ" if transaction['type'] == 'withdrawal' else "ì…ê¸ˆ"
        
        return template.format(
            date=transaction.get('date', ''),
            time=transaction.get('time', ''),
            type=transaction.get('type', ''),
            amount=transaction.get('amount', 0),
            description=transaction.get('description', ''),
            category=transaction.get('category', 'ê¸°íƒ€'),
            balance=transaction.get('balance', 0),
            action=action
        )
    
    def semantic_search(self, 
                       query: str, 
                       collection_name: str = 'transactions',
                       filter_dict: Optional[Dict] = None,
                       n_results: int = 10) -> List[Dict]:
        """ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰"""
        
        collection = self.collections.get(collection_name)
        if not collection:
            raise ValueError(f"Collection {collection_name} not found")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            where=filter_dict if filter_dict else None
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        if results['documents'] and results['documents'][0]:
            for i in range(len(results['documents'][0])):
                result = {
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                    'distance': results['distances'][0][i] if results['distances'] else 0,
                    'id': results['ids'][0][i] if results['ids'] else None
                }
                formatted_results.append(result)
        
        return formatted_results
    
    def add_customer_profile(self, user_id: str, profile_data: Dict):
        """ê³ ê° í”„ë¡œíŒŒì¼ ì¶”ê°€"""
        
        # í”„ë¡œíŒŒì¼ ë¬¸ì„œ ìƒì„±
        profile_doc = f"""
        ê³ ê° í”„ë¡œíŒŒì¼:
        ê³ ê° ID: {user_id}
        ì—°ë ¹ëŒ€: {profile_data.get('age_group', '')}
        ì§ì—…: {profile_data.get('occupation', '')}
        ì›” í‰ê·  ìˆ˜ì…: {profile_data.get('monthly_income', 0):,}ì›
        íˆ¬ì ì„±í–¥: {profile_data.get('risk_profile', '')}
        ê¸ˆìœµ ëª©í‘œ: {', '.join(profile_data.get('financial_goals', []))}
        
        ì„ í˜¸ ê¸ˆìœµ ìƒí’ˆ:
        {', '.join(profile_data.get('preferred_products', []))}
        
        ê±°ë˜ íŒ¨í„´:
        ì£¼ìš” ì§€ì¶œ ì¹´í…Œê³ ë¦¬: {profile_data.get('main_spending_category', '')}
        í‰ê·  ì›” ì§€ì¶œ: {profile_data.get('avg_monthly_spending', 0):,}ì›
        ì €ì¶•ë¥ : {profile_data.get('savings_rate', 0)}%
        """
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            'user_id': user_id,
            'age_group': profile_data.get('age_group', ''),
            'risk_profile': profile_data.get('risk_profile', ''),
            'monthly_income': profile_data.get('monthly_income', 0),
            'created_at': datetime.now().isoformat()
        }
        
        # í”„ë¡œíŒŒì¼ ì¶”ê°€
        self.collections['profiles'].add(
            documents=[profile_doc],
            metadatas=[metadata],
            ids=[user_id]
        )
    
    def get_similar_customers(self, user_id: str, n_results: int = 5) -> List[str]:
        """ìœ ì‚¬í•œ ê³ ê° ì°¾ê¸°"""
        
        # í˜„ì¬ ê³ ê° í”„ë¡œíŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        current_profile = self.collections['profiles'].get(ids=[user_id])
        
        if not current_profile['documents']:
            return []
        
        # ìœ ì‚¬ ê³ ê° ê²€ìƒ‰
        similar = self.collections['profiles'].query(
            query_texts=current_profile['documents'],
            n_results=n_results + 1,  # ìê¸° ìì‹  í¬í•¨
            where={"user_id": {"$ne": user_id}}  # ìê¸° ìì‹  ì œì™¸
        )
        
        similar_user_ids = []
        if similar['ids'] and similar['ids'][0]:
            similar_user_ids = [id for id in similar['ids'][0] if id != user_id]
        
        return similar_user_ids[:n_results]
    
    def delete_user_data(self, user_id: str):
        """ì‚¬ìš©ì ë°ì´í„° ì‚­ì œ (GDPR ì¤€ìˆ˜)"""
        
        # ê±°ë˜ ë°ì´í„° ì‚­ì œ
        trans_results = self.collections['transactions'].get(
            where={"user_id": user_id}
        )
        if trans_results['ids']:
            self.collections['transactions'].delete(ids=trans_results['ids'])
        
        # í”„ë¡œíŒŒì¼ ì‚­ì œ
        self.collections['profiles'].delete(ids=[user_id])
        
        return f"User {user_id} data deleted"

# ì‚¬ìš© ì˜ˆì‹œ
db = FinancialChromaDB()

# ê±°ë˜ ë°ì´í„° ì¶”ê°€
sample_transactions = [
    {
        'date': '2024-01-15',
        'time': '14:30',
        'type': 'withdrawal',
        'amount': 45000,
        'description': 'ì´ë§ˆíŠ¸ ì¥ë³´ê¸°',
        'category': 'ì‡¼í•‘',
        'balance': 2500000
    }
]

db.add_transactions_batch(sample_transactions, 'user123')

# ê²€ìƒ‰
results = db.semantic_search(
    "ì´ë§ˆíŠ¸ì—ì„œ ì‡¼í•‘í•œ ë‚´ì—­",
    filter_dict={'user_id': 'user123'}
)

for result in results:
    print(f"Score: {1 - result['distance']:.2f}")
    print(f"Document: {result['document'][:200]}...")
```

### 2.2 Pinecone ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬

```python
import pinecone
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Tuple
import json
from tqdm import tqdm

class PineconeFinancialIndex:
    """Pinecone ê¸°ë°˜ ëŒ€ê·œëª¨ ê¸ˆìœµ ë°ì´í„° ì¸ë±ìŠ¤"""
    
    def __init__(self, 
                 api_key: str,
                 environment: str = "us-west1-gcp",
                 index_name: str = "financial-rag"):
        
        # Pinecone ì´ˆê¸°í™”
        pinecone.init(api_key=api_key, environment=environment)
        
        # ì„ë² ë”© ëª¨ë¸ (multilingual ì§€ì›)
        self.encoder = SentenceTransformer('distiluse-base-multilingual-cased-v1')
        self.dimension = 512  # ëª¨ë¸ ì°¨ì›
        
        # ì¸ë±ìŠ¤ ì„¤ì •
        self.index_name = index_name
        self._setup_index()
    
    def _setup_index(self):
        """ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°"""
        
        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
        if self.index_name not in pinecone.list_indexes():
            # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
            pinecone.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric='cosine',
                pods=1,  # ë¬´ë£Œ í‹°ì–´
                replicas=1,
                pod_type='p1.x1'
            )
            print(f"Created new index: {self.index_name}")
        
        # ì¸ë±ìŠ¤ ì—°ê²°
        self.index = pinecone.Index(self.index_name)
        
        # í†µê³„ ì¶œë ¥
        stats = self.index.describe_index_stats()
        print(f"Index stats: {stats}")
    
    def bulk_upsert(self, 
                   documents: List[Dict],
                   batch_size: int = 100,
                   namespace: str = None):
        """ëŒ€ëŸ‰ ë°ì´í„° ì—…ì„œíŠ¸"""
        
        total_vectors = []
        
        for doc in tqdm(documents, desc="Processing documents"):
            # í…ìŠ¤íŠ¸ ìƒì„±
            text = self._document_to_text(doc)
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.encoder.encode(text).tolist()
            
            # ë²¡í„° êµ¬ì„±
            vector = {
                'id': doc['id'],
                'values': embedding,
                'metadata': self._prepare_metadata(doc)
            }
            
            total_vectors.append(vector)
            
            # ë°°ì¹˜ ì²˜ë¦¬
            if len(total_vectors) >= batch_size:
                self.index.upsert(
                    vectors=total_vectors,
                    namespace=namespace
                )
                total_vectors = []
        
        # ë‚¨ì€ ë²¡í„° ì²˜ë¦¬
        if total_vectors:
            self.index.upsert(
                vectors=total_vectors,
                namespace=namespace
            )
        
        print(f"Upserted {len(documents)} documents")
    
    def _document_to_text(self, doc: Dict) -> str:
        """ë¬¸ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        
        doc_type = doc.get('type', 'general')
        
        if doc_type == 'transaction':
            return f"""
            Transaction on {doc['date']}:
            Amount: {doc['amount']} KRW
            Category: {doc['category']}
            Description: {doc['description']}
            """
        
        elif doc_type == 'product':
            return f"""
            Financial Product: {doc['name']}
            Provider: {doc['provider']}
            Type: {doc['product_type']}
            Interest Rate: {doc.get('interest_rate', 'N/A')}
            Features: {', '.join(doc.get('features', []))}
            """
        
        elif doc_type == 'news':
            return f"""
            {doc['title']}
            {doc['content']}
            Source: {doc['source']}
            Date: {doc['date']}
            """
        
        else:
            return doc.get('text', str(doc))
    
    def _prepare_metadata(self, doc: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (40KB ì œí•œ)"""
        
        metadata = {}
        
        # í•„ìˆ˜ í•„ë“œ
        for key in ['type', 'date', 'user_id', 'category']:
            if key in doc:
                metadata[key] = doc[key]
        
        # ìˆ«ì í•„ë“œ
        for key in ['amount', 'score', 'priority']:
            if key in doc:
                metadata[key] = float(doc[key])
        
        # í…ìŠ¤íŠ¸ í•„ë“œ (ê¸¸ì´ ì œí•œ)
        for key in ['description', 'title']:
            if key in doc:
                metadata[key] = doc[key][:500]  # 500ì ì œí•œ
        
        return metadata
    
    def hybrid_search(self,
                     query: str,
                     filter_conditions: Dict = None,
                     namespace: str = None,
                     top_k: int = 10,
                     include_metadata: bool = True) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì‹œë§¨í‹± + ë©”íƒ€ë°ì´í„°)"""
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.encoder.encode(query).tolist()
        
        # Pinecone ê²€ìƒ‰
        results = self.index.query(
            vector=query_embedding,
            filter=filter_conditions,
            namespace=namespace,
            top_k=top_k,
            include_metadata=include_metadata
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for match in results['matches']:
            result = {
                'id': match['id'],
                'score': match['score'],
                'metadata': match.get('metadata', {})
            }
            formatted_results.append(result)
        
        return formatted_results
    
    def update_metadata(self, 
                       id: str,
                       metadata: Dict,
                       namespace: str = None):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        
        # ê¸°ì¡´ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        fetch_result = self.index.fetch(ids=[id], namespace=namespace)
        
        if id in fetch_result['vectors']:
            vector = fetch_result['vectors'][id]
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            vector['metadata'].update(metadata)
            
            # ë‹¤ì‹œ ì—…ì„œíŠ¸
            self.index.upsert(
                vectors=[{
                    'id': id,
                    'values': vector['values'],
                    'metadata': vector['metadata']
                }],
                namespace=namespace
            )
    
    def delete_by_filter(self, filter_conditions: Dict, namespace: str = None):
        """í•„í„° ì¡°ê±´ìœ¼ë¡œ ì‚­ì œ"""
        
        # ë”ë¯¸ ì¿¼ë¦¬ë¡œ í•„í„° ë§¤ì¹­ ID ì°¾ê¸°
        dummy_vector = [0.0] * self.dimension
        
        results = self.index.query(
            vector=dummy_vector,
            filter=filter_conditions,
            namespace=namespace,
            top_k=10000  # ìµœëŒ€í•œ ë§ì´
        )
        
        # ID ì¶”ì¶œ
        ids_to_delete = [match['id'] for match in results['matches']]
        
        # ì‚­ì œ
        if ids_to_delete:
            self.index.delete(ids=ids_to_delete, namespace=namespace)
            print(f"Deleted {len(ids_to_delete)} vectors")
        
        return len(ids_to_delete)

# ì‚¬ìš© ì˜ˆì‹œ
pinecone_index = PineconeFinancialIndex(
    api_key="your-api-key",
    environment="us-west1-gcp"
)

# ëŒ€ëŸ‰ ë°ì´í„° ì¤€ë¹„
documents = [
    {
        'id': f'doc_{i}',
        'type': 'transaction',
        'date': '2024-01-15',
        'amount': 50000 * i,
        'category': 'ì‡¼í•‘',
        'description': f'ê±°ë˜ {i}',
        'user_id': 'user123'
    }
    for i in range(1000)
]

# ì—…ì„œíŠ¸
pinecone_index.bulk_upsert(documents, batch_size=100)

# ê²€ìƒ‰
results = pinecone_index.hybrid_search(
    query="ëŒ€ëŸ‰ ì‡¼í•‘ ê±°ë˜",
    filter_conditions={'user_id': 'user123', 'category': 'ì‡¼í•‘'},
    top_k=5
)

for result in results:
    print(f"ID: {result['id']}, Score: {result['score']:.3f}")
```

---

## 3. RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì˜ˆì œ

### 3.1 LangChain RAG ì±—ë´‡

```python
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain.callbacks import StreamingStdOutCallbackHandler
import os

class FinancialAdvisorRAG:
    """ê¸ˆìœµ ìƒë‹´ RAG ì±—ë´‡"""
    
    def __init__(self, openai_api_key: str):
        os.environ["OPENAI_API_KEY"] = openai_api_key
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._init_embeddings()
        self._init_llm()
        self._init_vectorstore()
        self._init_memory()
        self._init_chain()
    
    def _init_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002"
        )
    
    def _init_llm(self):
        """LLM ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.2,
            max_tokens=1500,
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()]
        )
    
    def _init_vectorstore(self):
        """ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
        self.vectorstore = Chroma(
            persist_directory="./financial_vectorstore",
            embedding_function=self.embeddings,
            collection_name="financial_data"
        )
    
    def _init_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            k=5,  # ìµœê·¼ 5ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
            return_messages=True,
            output_key="answer"
        )
    
    def _init_chain(self):
        """RAG ì²´ì¸ ì´ˆê¸°í™”"""
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ê¸ˆìœµ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
        ê³ ê°ì˜ ê¸ˆìœµ ë°ì´í„°ì™€ ê±°ë˜ ë‚´ì—­ì„ ë¶„ì„í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
        
        ì°¸ê³  ì •ë³´:
        {context}
        
        ëŒ€í™” ê¸°ë¡:
        {chat_history}
        
        ê³ ê° ì§ˆë¬¸: {question}
        
        ë‹µë³€ ì§€ì¹¨:
        1. ì •í™•í•œ ë°ì´í„°ì™€ ìˆ˜ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
        2. ë³µì¡í•œ ê¸ˆìœµ ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…
        3. ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¡°ì–¸ ì œê³µ
        4. ë¦¬ìŠ¤í¬ê°€ ìˆëŠ” ì¡°ì–¸ì€ ì£¼ì˜ì‚¬í•­ ëª…ì‹œ
        5. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì „ë¬¸ê°€ ìƒë‹´ ê¶Œìœ 
        
        ë‹µë³€:"""
        
        qa_prompt = PromptTemplate(
            template=template,
            input_variables=["context", "chat_history", "question"]
        )
        
        # ì²´ì¸ ìƒì„±
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vectorstore.as_retriever(
                search_type="mmr",  # Maximum Marginal Relevance
                search_kwargs={"k": 5, "fetch_k": 10}
            ),
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": qa_prompt},
            return_source_documents=True,
            verbose=False
        )
    
    def chat(self, question: str) -> Dict:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬"""
        
        try:
            # RAG ì²´ì¸ ì‹¤í–‰
            result = self.qa_chain({"question": question})
            
            # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë¦¬
            sources = []
            if result.get("source_documents"):
                for doc in result["source_documents"]:
                    sources.append({
                        "content": doc.page_content[:200],
                        "metadata": doc.metadata
                    })
            
            return {
                "success": True,
                "answer": result["answer"],
                "sources": sources
            }
            
        except Exception as e:
            return {
                "success": False,
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": []
            }
    
    def analyze_spending(self, period: str = "1ê°œì›”") -> str:
        """ì§€ì¶œ ë¶„ì„"""
        question = f"""
        ìµœê·¼ {period}ê°„ì˜ ì§€ì¶œì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
        1. ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ ë¹„ì¤‘
        2. ì „ì›” ëŒ€ë¹„ ë³€í™”
        3. ë¹„ì •ìƒì ì¸ ì§€ì¶œ íŒ¨í„´
        4. ì ˆì•½ ê°€ëŠ¥ í•­ëª©
        5. êµ¬ì²´ì ì¸ ì ˆì•½ ë°©ë²•
        """
        return self.chat(question)["answer"]
    
    def recommend_products(self) -> str:
        """ê¸ˆìœµ ìƒí’ˆ ì¶”ì²œ"""
        question = """
        ë‚´ í˜„ì¬ ì¬ë¬´ ìƒí™©ì— ë§ëŠ” ê¸ˆìœµ ìƒí’ˆì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
        1. ì ê¸ˆ/ì˜ˆê¸ˆ ìƒí’ˆ
        2. íˆ¬ì ìƒí’ˆ
        3. ë³´í—˜ ìƒí’ˆ
        4. ê° ìƒí’ˆì˜ ì¥ë‹¨ì 
        5. ì˜ˆìƒ ìˆ˜ìµë¥ 
        """
        return self.chat(question)["answer"]
    
    def clear_memory(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.memory.clear()
        return "ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤."

# ì‚¬ìš© ì˜ˆì‹œ
advisor = FinancialAdvisorRAG(openai_api_key="your-key")

# ëŒ€í™”
response = advisor.chat("ì´ë²ˆ ë‹¬ ì¹´í˜ì—ì„œ ì–¼ë§ˆë‚˜ ì¼ë‚˜ìš”?")
print(f"ë‹µë³€: {response['answer']}")

# ì§€ì¶œ ë¶„ì„
analysis = advisor.analyze_spending("3ê°œì›”")
print(f"ë¶„ì„: {analysis}")
```

---

## 4. í†µí•© íŒŒì´í”„ë¼ì¸ ì˜ˆì œ

### 4.1 End-to-End RAG íŒŒì´í”„ë¼ì¸

```python
import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Optional
import logging
from datetime import datetime

class FinancialRAGPipeline:
    """ê¸ˆìœµ ë°ì´í„° End-to-End RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config_path: str = "config.json"):
        # ì„¤ì • ë¡œë“œ
        self.config = self._load_config(config_path)
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_converter = None
        self.vector_db = None
        self.rag_chatbot = None
        
        self._init_components()
    
    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        with open(config_path, 'r') as f:
            return json.load(f)
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rag_pipeline.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _init_components(self):
        """íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        
        # ë°ì´í„° ë³€í™˜ê¸°
        from data_converter import BankStatementConverter
        self.data_converter = BankStatementConverter(
            bank_name=self.config.get('bank_name', 'KBêµ­ë¯¼ì€í–‰')
        )
        
        # ë²¡í„° DB
        from vector_db import FinancialChromaDB
        self.vector_db = FinancialChromaDB(
            persist_directory=self.config.get('vector_db_path', './chroma_db')
        )
        
        # RAG ì±—ë´‡
        from rag_chatbot import FinancialAdvisorRAG
        self.rag_chatbot = FinancialAdvisorRAG(
            openai_api_key=self.config.get('openai_api_key')
        )
        
        self.logger.info("All components initialized")
    
    def process_csv_files(self, csv_folder: str, user_id: str) -> int:
        """CSV íŒŒì¼ ì¼ê´„ ì²˜ë¦¬"""
        
        csv_files = list(Path(csv_folder).glob("*.csv"))
        self.logger.info(f"Found {len(csv_files)} CSV files")
        
        total_transactions = 0
        
        for csv_file in csv_files:
            try:
                # CSV â†’ JSON ë³€í™˜
                json_data = self.data_converter.csv_to_json(str(csv_file))
                
                # ë²¡í„° DBì— ì €ì¥
                count = self.vector_db.add_transactions_batch(
                    json_data['transactions'],
                    user_id
                )
                
                total_transactions += count
                self.logger.info(f"Processed {csv_file.name}: {count} transactions")
                
            except Exception as e:
                self.logger.error(f"Error processing {csv_file.name}: {str(e)}")
        
        return total_transactions
    
    def create_user_profile(self, user_id: str, profile_data: Dict):
        """ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±"""
        
        try:
            # í”„ë¡œíŒŒì¼ ê²€ì¦
            required_fields = ['age_group', 'occupation', 'monthly_income']
            for field in required_fields:
                if field not in profile_data:
                    raise ValueError(f"Missing required field: {field}")
            
            # í”„ë¡œíŒŒì¼ ì €ì¥
            self.vector_db.add_customer_profile(user_id, profile_data)
            
            self.logger.info(f"Created profile for user {user_id}")
            
        except Exception as e:
            self.logger.error(f"Error creating profile: {str(e)}")
            raise
    
    def generate_financial_report(self, user_id: str) -> Dict:
        """ì¢…í•© ê¸ˆìœµ ë³´ê³ ì„œ ìƒì„±"""
        
        report = {
            'user_id': user_id,
            'generated_at': datetime.now().isoformat(),
            'sections': {}
        }
        
        try:
            # ì§€ì¶œ ë¶„ì„
            report['sections']['spending_analysis'] = self.rag_chatbot.analyze_spending("3ê°œì›”")
            
            # íˆ¬ì ì¶”ì²œ
            report['sections']['investment_recommendations'] = self.rag_chatbot.recommend_products()
            
            # ì ˆì•½ ë°©ì•ˆ
            savings_question = "êµ¬ì²´ì ì¸ ì ˆì•½ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”."
            report['sections']['savings_tips'] = self.rag_chatbot.chat(savings_question)['answer']
            
            # ì¬ë¬´ ê±´ê°•ë„
            health_question = "ë‚´ ì¬ë¬´ ê±´ê°•ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”."
            report['sections']['financial_health'] = self.rag_chatbot.chat(health_question)['answer']
            
            self.logger.info(f"Generated report for user {user_id}")
            
        except Exception as e:
            self.logger.error(f"Error generating report: {str(e)}")
            report['error'] = str(e)
        
        return report
    
    def interactive_session(self, user_id: str):
        """ëŒ€í™”í˜• ì„¸ì…˜ ì‹œì‘"""
        
        print("\n" + "="*50)
        print("AI ê¸ˆìœµ ìƒë‹´ì‚¬ì™€ì˜ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("="*50 + "\n")
        
        while True:
            try:
                # ì‚¬ìš©ì ì…ë ¥
                question = input("\nì§ˆë¬¸: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not question:
                    continue
                
                # RAG ì‘ë‹µ
                response = self.rag_chatbot.chat(question)
                
                print(f"\në‹µë³€: {response['answer']}")
                
                # ì†ŒìŠ¤ í‘œì‹œ
                if response.get('sources'):
                    print("\nì°¸ê³  ìë£Œ:")
                    for i, source in enumerate(response['sources'], 1):
                        print(f"  {i}. {source['content'][:100]}...")
                
            except KeyboardInterrupt:
                print("\n\nëŒ€í™”ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            
            except Exception as e:
                print(f"\nì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def run_full_pipeline(self, user_id: str, csv_folder: str, profile_data: Dict):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        self.logger.info(f"Starting full pipeline for user {user_id}")
        
        # 1. CSV íŒŒì¼ ì²˜ë¦¬
        print("1. CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
        transaction_count = self.process_csv_files(csv_folder, user_id)
        print(f"   âœ… {transaction_count}ê°œ ê±°ë˜ ì²˜ë¦¬ ì™„ë£Œ")
        
        # 2. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„±
        print("2. ì‚¬ìš©ì í”„ë¡œíŒŒì¼ ìƒì„± ì¤‘...")
        self.create_user_profile(user_id, profile_data)
        print("   âœ… í”„ë¡œíŒŒì¼ ìƒì„± ì™„ë£Œ")
        
        # 3. ê¸ˆìœµ ë³´ê³ ì„œ ìƒì„±
        print("3. ê¸ˆìœµ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        report = self.generate_financial_report(user_id)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = f"reports/{user_id}_report_{datetime.now().strftime('%Y%m%d')}.json"
        Path("reports").mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        # 4. ëŒ€í™”í˜• ì„¸ì…˜
        print("\n4. AI ìƒë‹´ ì‹œì‘")
        self.interactive_session(user_id)
        
        self.logger.info(f"Pipeline completed for user {user_id}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì„¤ì • íŒŒì¼ ìƒì„±
    config = {
        "bank_name": "KBêµ­ë¯¼ì€í–‰",
        "vector_db_path": "./financial_chroma_db",
        "openai_api_key": "your-openai-api-key"
    }
    
    with open("config.json", "w") as f:
        json.dump(config, f)
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = FinancialRAGPipeline("config.json")
    
    # ì‚¬ìš©ì í”„ë¡œíŒŒì¼
    profile = {
        "age_group": "30ëŒ€",
        "occupation": "íšŒì‚¬ì›",
        "monthly_income": 4000000,
        "risk_profile": "ì¤‘ë¦½í˜•",
        "financial_goals": ["ì£¼íƒ êµ¬ë§¤", "ë…¸í›„ ì¤€ë¹„"],
        "preferred_products": ["ì ê¸ˆ", "í€ë“œ"]
    }
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline.run_full_pipeline(
        user_id="user123",
        csv_folder="./data/csv_files",
        profile_data=profile
    )
```

---

## ì½”ë“œ ì‹¤í–‰ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### requirements.txt

```
pandas==2.0.3
numpy==1.24.3
chromadb==0.4.22
pinecone-client==2.2.4
langchain==0.1.0
langchain-openai==0.0.5
sentence-transformers==2.2.2
openai==1.6.1
matplotlib==3.7.1
seaborn==0.12.2
tqdm==4.66.1
python-dotenv==1.0.0
```

### ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

```python
# run_pipeline.py

from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
from financial_rag_pipeline import FinancialRAGPipeline

pipeline = FinancialRAGPipeline()
pipeline.run_full_pipeline(
    user_id="test_user",
    csv_folder="./sample_data",
    profile_data={
        "age_group": "30ëŒ€",
        "occupation": "ê°œë°œì",
        "monthly_income": 5000000
    }
)
```

---

## ì£¼ì˜ì‚¬í•­

1. **API í‚¤ ê´€ë¦¬**: ì ˆëŒ€ ì½”ë“œì— ì§ì ‘ API í‚¤ë¥¼ ë„£ì§€ ë§ˆì„¸ìš”
2. **ë°ì´í„° ë³´ì•ˆ**: ê¸ˆìœµ ë°ì´í„°ëŠ” ì•”í˜¸í™”í•˜ì—¬ ì €ì¥í•˜ì„¸ìš”
3. **ê·œì œ ì¤€ìˆ˜**: GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²•ì„ ì¤€ìˆ˜í•˜ì„¸ìš”
4. **ë¹„ìš© ê´€ë¦¬**: OpenAI API ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”
5. **ì—ëŸ¬ ì²˜ë¦¬**: ëª¨ë“  ì˜ˆì™¸ ìƒí™©ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•˜ì„¸ìš”