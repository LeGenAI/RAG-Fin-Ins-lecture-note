# 2-2. ê°œì¸ ê±°ë˜ë‚´ì—­ ë° ë³´í—˜ìë£Œ í…ìŠ¤íŠ¸ë¥¼ .md, .json í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸°

## í•™ìŠµ ëª©í‘œ
- ê¸ˆìœµ ê±°ë˜ë‚´ì—­ ë°ì´í„°ì˜ êµ¬ì¡°í™” ë°©ë²• ì´í•´
- CSV/Excel ë°ì´í„°ë¥¼ JSON/Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‹¤ë¬´ ê¸°ë²• ìŠµë“
- ë³´í—˜ ë°ì´í„° ë³€í™˜ ë° Python ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©ë²• ë§ˆìŠ¤í„°

---

## 1. ê¸ˆìœµ ê±°ë˜ë‚´ì—­ ë°ì´í„° êµ¬ì¡°í™”

### 1.1 ì™œ CSV/Excelì—ì„œ JSON/Markdownìœ¼ë¡œ ë³€í™˜í•˜ëŠ”ê°€?

#### AI ëª¨ë¸ í˜¸í™˜ì„± í–¥ìƒ

**ì „í†µì  í˜•ì‹ì˜ í•œê³„**
```csv
2024-01-15,14:30:25,1001,4500,,2450000,01
```
- AIê°€ ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ì½”ë“œê°’
- ë§¥ë½ ì •ë³´ ë¶€ì¬
- ê´€ê³„ì„± í‘œí˜„ ë¶ˆê°€

**AI-Ready í˜•ì‹ì˜ ì¥ì **
```json
{
  "transaction": {
    "narrative": "2024ë…„ 1ì›” 15ì¼ ì˜¤í›„ 2ì‹œ 30ë¶„, ìŠ¤íƒ€ë²…ìŠ¤ì—ì„œ ì»¤í”¼ë¥¼ êµ¬ë§¤í–ˆìŠµë‹ˆë‹¤.",
    "context": "í‰ì¼ ì ì‹¬ì‹œê°„ ì´í›„ ì¼ìƒì ì¸ ì¹´í˜ ë°©ë¬¸",
    "amount": 4500,
    "category": "ì‹ìŒë£Œ",
    "frequency": "ë§¤ì¼"
  }
}
```

#### ë³€í™˜ì˜ í•µì‹¬ ì´ì 

| ì¸¡ë©´ | CSV/Excel | JSON/Markdown |
|------|-----------|---------------|
| **ê°€ë…ì„±** | ìˆ«ìì™€ ì½”ë“œ ì¤‘ì‹¬ | ìì—°ì–´ ì„¤ëª… í¬í•¨ |
| **í™•ì¥ì„±** | ê³ ì •ëœ ì»¬ëŸ¼ êµ¬ì¡° | ìœ ì—°í•œ ì¤‘ì²© êµ¬ì¡° |
| **ë©”íƒ€ë°ì´í„°** | ì œí•œì  | í’ë¶€í•œ ë¶€ê°€ì •ë³´ |
| **AI í•™ìŠµ** | ì „ì²˜ë¦¬ í•„ìš” | ì¦‰ì‹œ í™œìš© ê°€ëŠ¥ |

### 1.2 ì‹¤ì œ ì€í–‰ ê±°ë˜ë‚´ì—­ í¬ë§· ë¶„ì„

#### KBêµ­ë¯¼ì€í–‰ ê±°ë˜ë‚´ì—­ êµ¬ì¡°

**ì›ë³¸ CSV í˜•ì‹**
```csv
ê±°ë˜ì¼ì,ê±°ë˜ì‹œê°„,ì ìš”,ì¶œê¸ˆì•¡,ì…ê¸ˆì•¡,ì”ì•¡,ê±°ë˜ì 
2024-01-15,14:30:25,ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ,4500,,2450000,ì¸í„°ë„·ë±…í‚¹
2024-01-15,09:15:10,ê¸‰ì—¬,,2800000,2454500,ìë™ì´ì²´
2024-01-14,19:22:15,ì´ë§ˆíŠ¸ ì›”ë“œì»µì ,127500,,2327000,ì²´í¬ì¹´ë“œ
```

**ë³€í™˜ëœ JSON êµ¬ì¡°**
```json
{
  "account": {
    "bank": "KBêµ­ë¯¼ì€í–‰",
    "account_number": "***-****-****-***",
    "currency": "KRW"
  },
  "transactions": [
    {
      "id": "KB2024011514302501",
      "datetime": "2024-01-15T14:30:25+09:00",
      "type": "withdrawal",
      "amount": 4500,
      "balance_after": 2450000,
      "merchant": {
        "name": "ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ",
        "category": "ì¹´í˜",
        "mcc": "5814"
      },
      "channel": "ì¸í„°ë„·ë±…í‚¹",
      "tags": ["ì¼ìƒì†Œë¹„", "ì‹ìŒë£Œ", "ì •ê¸°ì§€ì¶œ"],
      "ai_insights": {
        "spending_pattern": "regular_daily",
        "budget_impact": "within_limit",
        "category_rank": 3
      }
    }
  ]
}
```

#### ì‹ í•œì€í–‰ SOL ê±°ë˜ë‚´ì—­

**Excel ë‹¤ìš´ë¡œë“œ í˜•ì‹**
```
ê±°ë˜ì¼|ê±°ë˜êµ¬ë¶„|ê±°ë˜ì ëª…|ì ìš”|ì¶œê¸ˆì•¡|ì…ê¸ˆì•¡|ê±°ë˜í›„ì”ì•¡|ê±°ë˜ì‹œê°„
2024-01-15|ì´ì²´|ì˜¨ë¼ì¸|ì¹´ì¹´ì˜¤í˜ì´|15000||3450000|15:45:22
```

**Markdown ë³€í™˜ ì˜ˆì‹œ**
```markdown
## ê±°ë˜ ë‚´ì—­ ìš”ì•½

### 2024ë…„ 1ì›” 15ì¼
**ì˜¤í›„ 3:45** | ì¹´ì¹´ì˜¤í˜ì´ ì†¡ê¸ˆ
- ê¸ˆì•¡: 15,000ì› (ì¶œê¸ˆ)
- ì”ì•¡: 3,450,000ì›
- ì±„ë„: ì˜¨ë¼ì¸ ì´ì²´
- ë¶„ë¥˜: P2P ì†¡ê¸ˆ / ì¼ìƒ ê±°ë˜
```

#### ìš°ë¦¬ì€í–‰ WONë±…í‚¹ ë°ì´í„°

**êµ¬ì¡°í™”ëœ JSON-LD í˜•ì‹**
```json
{
  "@context": "https://schema.org",
  "@type": "FinancialTransaction",
  "identifier": "WR20240115123456",
  "transactionDate": "2024-01-15",
  "amount": {
    "@type": "MonetaryAmount",
    "value": 50000,
    "currency": "KRW"
  },
  "description": "êµí†µë¹„ ì •ì‚°",
  "category": "Transportation",
  "paymentMethod": {
    "@type": "PaymentCard",
    "name": "ìš°ë¦¬ì¹´ë“œ"
  }
}
```

---

## 2. ë³´í—˜ ë°ì´í„° ë³€í™˜ ì‚¬ë¡€

### 2.1 ë³´í—˜ ì•½ê´€ í…ìŠ¤íŠ¸ êµ¬ì¡°í™”

#### ì‚¼ì„±ìƒëª… ë³´í—˜ ì•½ê´€ ë³€í™˜

**ì›ë³¸ ì•½ê´€ í…ìŠ¤íŠ¸**
```text
ì œ10ì¡°(ë³´í—˜ê¸ˆì˜ ì§€ê¸‰)
â‘  íšŒì‚¬ëŠ” í”¼ë³´í—˜ìê°€ ì´ ì•½ê´€ì—ì„œ ì •í•œ ë³´í—˜ê¸ˆ ì§€ê¸‰ì‚¬ìœ ê°€ ë°œìƒí•œ ë•Œì—ëŠ” 
   ìˆ˜ìµìì—ê²Œ ì•½ì •í•œ ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•©ë‹ˆë‹¤.
â‘¡ ì œ1í•­ì˜ ë³´í—˜ê¸ˆ ì§€ê¸‰ì‚¬ìœ ì˜ ë°œìƒì‹œê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
   1. ì‚¬ë§ë³´í—˜ê¸ˆ: í”¼ë³´í—˜ìê°€ ì‚¬ë§í•œ ë•Œ
   2. ë§Œê¸°ë³´í—˜ê¸ˆ: ë³´í—˜ê¸°ê°„ì´ ëë‚˜ëŠ” ë•Œ
```

**êµ¬ì¡°í™”ëœ JSON ë³€í™˜**
```json
{
  "policy": {
    "company": "ì‚¼ì„±ìƒëª…",
    "product": "ì¢…ì‹ ë³´í—˜",
    "version": "2024.01"
  },
  "clauses": [
    {
      "article_number": 10,
      "title": "ë³´í—˜ê¸ˆì˜ ì§€ê¸‰",
      "sections": [
        {
          "id": "10-1",
          "content": "íšŒì‚¬ëŠ” í”¼ë³´í—˜ìê°€ ì´ ì•½ê´€ì—ì„œ ì •í•œ ë³´í—˜ê¸ˆ ì§€ê¸‰ì‚¬ìœ ê°€ ë°œìƒí•œ ë•Œì—ëŠ” ìˆ˜ìµìì—ê²Œ ì•½ì •í•œ ë³´í—˜ê¸ˆì„ ì§€ê¸‰í•©ë‹ˆë‹¤.",
          "keywords": ["ë³´í—˜ê¸ˆ", "ì§€ê¸‰", "í”¼ë³´í—˜ì", "ìˆ˜ìµì"],
          "conditions": {
            "trigger": "ì§€ê¸‰ì‚¬ìœ  ë°œìƒ",
            "beneficiary": "ìˆ˜ìµì",
            "obligation": "mandatory"
          }
        }
      ],
      "payment_events": [
        {
          "type": "death_benefit",
          "condition": "í”¼ë³´í—˜ì ì‚¬ë§",
          "payment_to": "ìˆ˜ìµì",
          "verification_required": ["ì‚¬ë§ì§„ë‹¨ì„œ", "ê¸°ë³¸ì¦ëª…ì„œ"]
        },
        {
          "type": "maturity_benefit",
          "condition": "ë³´í—˜ê¸°ê°„ ë§Œë£Œ",
          "payment_to": "ê³„ì•½ì",
          "verification_required": ["ì‹ ë¶„ì¦", "í†µì¥ì‚¬ë³¸"]
        }
      ]
    }
  ]
}
```

### 2.2 ë³´í—˜ê¸ˆ ì²­êµ¬ ë°ì´í„° ë³€í™˜

#### KBì†í•´ë³´í—˜ ì²­êµ¬ ë°ì´í„°

**ì›ë³¸ ì²­êµ¬ì„œ ë°ì´í„°**
```
ì²­êµ¬ë²ˆí˜¸: KB2024-CLM-00123
ì²­êµ¬ì¼ì: 2024-01-15
ì²­êµ¬ìœ í˜•: ì‹¤ì†ì˜ë£Œë¹„
ì§„ë£Œê¸°ê´€: ì„œìš¸ëŒ€í•™êµë³‘ì›
ì§„ë£Œê¸°ê°„: 2024-01-10 ~ 2024-01-12
ì²­êµ¬ê¸ˆì•¡: 850,000ì›
```

**AI ì²˜ë¦¬ìš© JSON ë³€í™˜**
```json
{
  "claim": {
    "id": "KB2024-CLM-00123",
    "submitted_date": "2024-01-15",
    "type": "medical_expense",
    "status": "under_review",
    "medical_info": {
      "hospital": "ì„œìš¸ëŒ€í•™êµë³‘ì›",
      "treatment_period": {
        "start": "2024-01-10",
        "end": "2024-01-12"
      },
      "diagnosis": {
        "code": "K35.8",
        "description": "ê¸‰ì„±ì¶©ìˆ˜ì—¼"
      }
    },
    "financial_info": {
      "total_expense": 850000,
      "claimed_amount": 680000,
      "deductible": 170000,
      "expected_reimbursement": 544000
    },
    "documents": [
      {
        "type": "receipt",
        "status": "verified",
        "ocr_extracted": true
      },
      {
        "type": "diagnosis",
        "status": "verified",
        "ai_validated": true
      }
    ],
    "ai_assessment": {
      "fraud_score": 0.02,
      "auto_approval": true,
      "processing_time": "instant"
    }
  }
}
```

### 2.3 í•œí™”ìƒëª… ë³´í—˜ ìƒí’ˆ ë°ì´í„°

**Markdown í˜•ì‹ ìƒí’ˆ ì„¤ëª…ì„œ**
```markdown
# í•œí™”ìƒëª… eë“ ë“ í•œ ì¢…ì‹ ë³´í—˜

## ìƒí’ˆ ê°œìš”
- **ìƒí’ˆìœ í˜•**: ì¢…ì‹ ë³´í—˜
- **ë³´í—˜ê¸°ê°„**: ì¢…ì‹ 
- **ë‚©ì…ê¸°ê°„**: 10ë…„/15ë…„/20ë…„/30ë…„ ì„ íƒ
- **ê°€ì…ì—°ë ¹**: 15ì„¸ ~ 70ì„¸

## ì£¼ìš” ë³´ì¥ë‚´ìš©

### ì‚¬ë§ë³´í—˜ê¸ˆ
í”¼ë³´í—˜ìê°€ ë³´í—˜ê¸°ê°„ ì¤‘ ì‚¬ë§ì‹œ **1ì–µì›** ì§€ê¸‰

### íŠ¹ì•½ ì„ íƒì‚¬í•­
1. **ì•”ì§„ë‹¨íŠ¹ì•½**: ì•” ì§„ë‹¨ì‹œ 3ì²œë§Œì›
2. **ë‡Œì¶œí˜ˆì§„ë‹¨íŠ¹ì•½**: ë‡Œì¶œí˜ˆ ì§„ë‹¨ì‹œ 2ì²œë§Œì›
3. **ê¸‰ì„±ì‹¬ê·¼ê²½ìƒ‰ì§„ë‹¨íŠ¹ì•½**: ê¸‰ì„±ì‹¬ê·¼ê²½ìƒ‰ ì§„ë‹¨ì‹œ 2ì²œë§Œì›

## ë³´í—˜ë£Œ ì˜ˆì‹œ
| ë‚˜ì´ | ì„±ë³„ | ì›”ë³´í—˜ë£Œ |
|------|------|----------|
| 30ì„¸ | ë‚¨ì | 125,000ì› |
| 30ì„¸ | ì—¬ì | 108,000ì› |
| 40ì„¸ | ë‚¨ì | 168,000ì› |
| 40ì„¸ | ì—¬ì | 145,000ì› |

## AI ì¶”ì²œ í¬ì¸íŠ¸
- 30ëŒ€ ê°€ì¥ì—ê²Œ ì í•©í•œ ê¸°ë³¸ ë³´ì¥
- íŠ¹ì•½ ì¡°í•©ìœ¼ë¡œ ë§ì¶¤í˜• ì„¤ê³„ ê°€ëŠ¥
- ì—…ê³„ í‰ê·  ëŒ€ë¹„ ê²½ìŸë ¥ ìˆëŠ” ë³´í—˜ë£Œ
```

---

## 3. Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë„êµ¬

### 3.1 ê¸°ë³¸ ë°ì´í„° ë³€í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### pandasë¥¼ í™œìš©í•œ CSV ì²˜ë¦¬

```python
import pandas as pd
import json
from datetime import datetime

class BankDataProcessor:
    def __init__(self, bank_name):
        self.bank_name = bank_name
        self.encoding_map = {
            'KBêµ­ë¯¼ì€í–‰': 'cp949',
            'ì‹ í•œì€í–‰': 'utf-8',
            'ìš°ë¦¬ì€í–‰': 'euc-kr',
            'í•˜ë‚˜ì€í–‰': 'cp949'
        }
    
    def load_csv(self, file_path):
        """ì€í–‰ë³„ ì¸ì½”ë”©ì„ ê³ ë ¤í•œ CSV ë¡œë“œ"""
        encoding = self.encoding_map.get(self.bank_name, 'utf-8')
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            return df
        except UnicodeDecodeError:
            # ì¸ì½”ë”© ìë™ ê°ì§€
            import chardet
            with open(file_path, 'rb') as f:
                result = chardet.detect(f.read())
            df = pd.read_csv(file_path, encoding=result['encoding'])
            return df
    
    def standardize_columns(self, df):
        """ì€í–‰ë³„ ì»¬ëŸ¼ëª… í‘œì¤€í™”"""
        column_mapping = {
            'ê±°ë˜ì¼ì': 'date',
            'ê±°ë˜ì¼': 'date',
            'ì¼ì': 'date',
            'ê±°ë˜ì‹œê°„': 'time',
            'ì‹œê°„': 'time',
            'ì ìš”': 'description',
            'ê±°ë˜ë‚´ìš©': 'description',
            'ì¶œê¸ˆì•¡': 'withdrawal',
            'ì¶œê¸ˆ': 'withdrawal',
            'ì…ê¸ˆì•¡': 'deposit',
            'ì…ê¸ˆ': 'deposit',
            'ì”ì•¡': 'balance',
            'ê±°ë˜í›„ì”ì•¡': 'balance'
        }
        
        df_renamed = df.rename(columns=column_mapping)
        return df_renamed
    
    def enrich_transaction_data(self, df):
        """ê±°ë˜ ë°ì´í„° enrichment"""
        # ê±°ë˜ ìœ í˜• ì¶”ê°€
        df['transaction_type'] = df.apply(
            lambda x: 'withdrawal' if pd.notna(x.get('withdrawal', 0)) and x.get('withdrawal', 0) > 0 
            else 'deposit', axis=1
        )
        
        # ê±°ë˜ ê¸ˆì•¡ í†µí•©
        df['amount'] = df.apply(
            lambda x: x.get('withdrawal', 0) if pd.notna(x.get('withdrawal', 0)) 
            else x.get('deposit', 0), axis=1
        )
        
        # ë‚ ì§œ/ì‹œê°„ íŒŒì‹±
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df['weekday'] = df['date'].dt.day_name()
            df['month'] = df['date'].dt.month
            df['year'] = df['date'].dt.year
        
        return df

# ì‚¬ìš© ì˜ˆì‹œ
processor = BankDataProcessor('KBêµ­ë¯¼ì€í–‰')
df = processor.load_csv('kb_transactions.csv')
df = processor.standardize_columns(df)
df = processor.enrich_transaction_data(df)
```

#### JSON ë³€í™˜ ê³ ê¸‰ ê¸°ë²•

```python
import json
from decimal import Decimal
from datetime import datetime, date

class FinancialJSONEncoder(json.JSONEncoder):
    """ê¸ˆìœµ ë°ì´í„°ìš© ì»¤ìŠ¤í…€ JSON ì¸ì½”ë”"""
    
    def default(self, obj):
        if isinstance(obj, Decimal):
            return float(obj)
        elif isinstance(obj, (datetime, date)):
            return obj.isoformat()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif pd.isna(obj):
            return None
        return super().default(obj)

def create_transaction_json(df, user_id, account_number):
    """DataFrameì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜"""
    
    transactions = []
    for _, row in df.iterrows():
        transaction = {
            'id': f"{user_id}_{row.name}",
            'date': row['date'],
            'time': row.get('time', '00:00:00'),
            'description': row['description'],
            'amount': abs(row['amount']),
            'type': row['transaction_type'],
            'balance_after': row['balance'],
            'metadata': {
                'weekday': row.get('weekday'),
                'month': row.get('month'),
                'year': row.get('year'),
                'channel': row.get('channel', 'unknown')
            },
            'ai_classification': classify_transaction(row['description'])
        }
        transactions.append(transaction)
    
    result = {
        'account': {
            'user_id': user_id,
            'account_number': account_number,
            'bank': 'KBêµ­ë¯¼ì€í–‰',
            'currency': 'KRW'
        },
        'summary': {
            'total_transactions': len(transactions),
            'total_deposits': df[df['transaction_type'] == 'deposit']['amount'].sum(),
            'total_withdrawals': df[df['transaction_type'] == 'withdrawal']['amount'].sum(),
            'period': {
                'start': df['date'].min(),
                'end': df['date'].max()
            }
        },
        'transactions': transactions
    }
    
    return json.dumps(result, cls=FinancialJSONEncoder, ensure_ascii=False, indent=2)

def classify_transaction(description):
    """AIë¥¼ ìœ„í•œ ê±°ë˜ ìë™ ë¶„ë¥˜"""
    categories = {
        'ì‹ìŒë£Œ': ['ì¹´í˜', 'ì»¤í”¼', 'ìŠ¤íƒ€ë²…ìŠ¤', 'ì‹ë‹¹', 'ë°°ë‹¬'],
        'êµí†µ': ['ì§€í•˜ì² ', 'ë²„ìŠ¤', 'íƒì‹œ', 'ì£¼ìœ ', 'ì£¼ì°¨'],
        'ì‡¼í•‘': ['ë§ˆíŠ¸', 'ë°±í™”ì ', 'ì˜¨ë¼ì¸', 'ì¿ íŒ¡', 'ë„¤ì´ë²„'],
        'ì˜ë£Œ': ['ë³‘ì›', 'ì•½êµ­', 'ì˜ì›', 'í•œì˜ì›'],
        'í†µì‹ ': ['KT', 'SKT', 'LG', 'í†µì‹ '],
        'ê¸ˆìœµ': ['ì´ì²´', 'ëŒ€ì¶œ', 'ì´ì', 'ìˆ˜ìˆ˜ë£Œ'],
        'ê¸‰ì—¬': ['ê¸‰ì—¬', 'ì›”ê¸‰', 'ìƒì—¬', 'ë³´ë„ˆìŠ¤']
    }
    
    description_lower = description.lower()
    for category, keywords in categories.items():
        if any(keyword in description_lower for keyword in keywords):
            return {
                'category': category,
                'confidence': 0.95,
                'keywords_matched': [k for k in keywords if k in description_lower]
            }
    
    return {
        'category': 'ê¸°íƒ€',
        'confidence': 0.5,
        'keywords_matched': []
    }
```

### 3.2 Markdown ë³€í™˜ ë„êµ¬

```python
import pandas as pd
from datetime import datetime
from typing import List, Dict

class MarkdownReportGenerator:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.report = []
    
    def add_header(self, text: str, level: int = 1):
        """í—¤ë” ì¶”ê°€"""
        self.report.append(f"{'#' * level} {text}\n")
    
    def add_paragraph(self, text: str):
        """ë‹¨ë½ ì¶”ê°€"""
        self.report.append(f"{text}\n")
    
    def add_table(self, data: pd.DataFrame, columns: List[str] = None):
        """í…Œì´ë¸” ì¶”ê°€"""
        if columns:
            data = data[columns]
        
        # í—¤ë”
        headers = '| ' + ' | '.join(data.columns) + ' |'
        separator = '|' + '|'.join(['---' for _ in data.columns]) + '|'
        
        self.report.append(headers)
        self.report.append(separator)
        
        # ë°ì´í„° í–‰
        for _, row in data.iterrows():
            row_str = '| ' + ' | '.join([str(v) for v in row.values]) + ' |'
            self.report.append(row_str)
        
        self.report.append('')
    
    def add_list(self, items: List[str], ordered: bool = False):
        """ë¦¬ìŠ¤íŠ¸ ì¶”ê°€"""
        for i, item in enumerate(items):
            prefix = f"{i+1}." if ordered else "-"
            self.report.append(f"{prefix} {item}")
        self.report.append('')
    
    def generate_financial_report(self):
        """ê¸ˆìœµ ê±°ë˜ ë³´ê³ ì„œ ìƒì„±"""
        self.add_header("ê¸ˆìœµ ê±°ë˜ ë¶„ì„ ë³´ê³ ì„œ")
        
        # ê¸°ê°„ ì •ë³´
        start_date = self.df['date'].min()
        end_date = self.df['date'].max()
        self.add_paragraph(f"**ë¶„ì„ ê¸°ê°„**: {start_date} ~ {end_date}")
        
        # ìš”ì•½ í†µê³„
        self.add_header("ê±°ë˜ ìš”ì•½", 2)
        
        summary_items = [
            f"ì´ ê±°ë˜ ê±´ìˆ˜: **{len(self.df)}ê±´**",
            f"ì´ ì…ê¸ˆì•¡: **{self.df[self.df['transaction_type']=='deposit']['amount'].sum():,.0f}ì›**",
            f"ì´ ì¶œê¸ˆì•¡: **{self.df[self.df['transaction_type']=='withdrawal']['amount'].sum():,.0f}ì›**",
            f"í‰ê·  ê±°ë˜ì•¡: **{self.df['amount'].mean():,.0f}ì›**"
        ]
        self.add_list(summary_items)
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        self.add_header("ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ ë¶„ì„", 2)
        
        if 'category' in self.df.columns:
            category_summary = self.df[self.df['transaction_type']=='withdrawal'].groupby('category')['amount'].agg(['sum', 'count', 'mean'])
            category_summary.columns = ['ì´ì•¡', 'ê±´ìˆ˜', 'í‰ê· ']
            category_summary['ì´ì•¡'] = category_summary['ì´ì•¡'].apply(lambda x: f"{x:,.0f}ì›")
            category_summary['í‰ê· '] = category_summary['í‰ê· '].apply(lambda x: f"{x:,.0f}ì›")
            
            self.add_table(category_summary.reset_index())
        
        # ì£¼ìš” ê±°ë˜ ë‚´ì—­
        self.add_header("ìµœê·¼ ì£¼ìš” ê±°ë˜", 2)
        
        recent_transactions = self.df.nlargest(5, 'amount')[['date', 'description', 'amount', 'transaction_type']]
        recent_transactions['amount'] = recent_transactions['amount'].apply(lambda x: f"{x:,.0f}ì›")
        
        self.add_table(recent_transactions)
        
        # AI ì¸ì‚¬ì´íŠ¸
        self.add_header("AI ë¶„ì„ ì¸ì‚¬ì´íŠ¸", 2)
        
        insights = self.generate_ai_insights()
        self.add_list(insights)
        
        return '\n'.join(self.report)
    
    def generate_ai_insights(self):
        """AI ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì§€ì¶œ íŒ¨í„´ ë¶„ì„
        if 'weekday' in self.df.columns:
            top_spending_day = self.df[self.df['transaction_type']=='withdrawal'].groupby('weekday')['amount'].sum().idxmax()
            insights.append(f"ğŸ“Š **{top_spending_day}**ì— ê°€ì¥ ë§ì€ ì§€ì¶œì´ ë°œìƒí•©ë‹ˆë‹¤")
        
        # ì›”ë³„ íŠ¸ë Œë“œ
        if 'month' in self.df.columns:
            monthly_spending = self.df[self.df['transaction_type']=='withdrawal'].groupby('month')['amount'].sum()
            if len(monthly_spending) > 1:
                trend = "ì¦ê°€" if monthly_spending.iloc[-1] > monthly_spending.iloc[0] else "ê°ì†Œ"
                insights.append(f"ğŸ“ˆ ì›”ë³„ ì§€ì¶œì´ **{trend}** ì¶”ì„¸ë¥¼ ë³´ì…ë‹ˆë‹¤")
        
        # ê³ ì•¡ ê±°ë˜ ì•Œë¦¼
        high_value_threshold = self.df['amount'].quantile(0.9)
        high_value_count = len(self.df[self.df['amount'] > high_value_threshold])
        insights.append(f"âš ï¸ {high_value_count}ê±´ì˜ ê³ ì•¡ ê±°ë˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ (ê¸°ì¤€: {high_value_threshold:,.0f}ì› ì´ìƒ)")
        
        return insights

# ì‚¬ìš© ì˜ˆì‹œ
generator = MarkdownReportGenerator(df)
markdown_report = generator.generate_financial_report()
print(markdown_report)

# íŒŒì¼ë¡œ ì €ì¥
with open('financial_report.md', 'w', encoding='utf-8') as f:
    f.write(markdown_report)
```

### 3.3 ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬

```python
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import logging

class DataQualityValidator:
    def __init__(self):
        self.validation_results = {}
        self.logger = logging.getLogger(__name__)
    
    def validate_completeness(self, df: pd.DataFrame, required_columns: List[str]) -> Dict:
        """ë°ì´í„° ì™„ì „ì„± ê²€ì¦"""
        results = {}
        
        for col in required_columns:
            if col not in df.columns:
                results[col] = {
                    'exists': False,
                    'completeness': 0.0,
                    'message': f"Required column '{col}' is missing"
                }
            else:
                missing_count = df[col].isna().sum()
                total_count = len(df)
                completeness = 1 - (missing_count / total_count)
                
                results[col] = {
                    'exists': True,
                    'completeness': completeness,
                    'missing_count': missing_count,
                    'total_count': total_count,
                    'status': 'OK' if completeness > 0.95 else 'WARNING' if completeness > 0.8 else 'ERROR'
                }
        
        return results
    
    def validate_consistency(self, df: pd.DataFrame) -> Dict:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        results = {}
        
        # ì”ì•¡ ì¼ê´€ì„± ê²€ì‚¬
        if all(col in df.columns for col in ['balance', 'amount', 'transaction_type']):
            balance_errors = []
            
            for i in range(1, len(df)):
                prev_balance = df.iloc[i-1]['balance']
                curr_balance = df.iloc[i]['balance']
                transaction_amount = df.iloc[i]['amount']
                transaction_type = df.iloc[i]['transaction_type']
                
                if transaction_type == 'withdrawal':
                    expected_balance = prev_balance - transaction_amount
                else:
                    expected_balance = prev_balance + transaction_amount
                
                if abs(expected_balance - curr_balance) > 1:  # 1ì› ì˜¤ì°¨ í—ˆìš©
                    balance_errors.append({
                        'index': i,
                        'expected': expected_balance,
                        'actual': curr_balance,
                        'difference': expected_balance - curr_balance
                    })
            
            results['balance_consistency'] = {
                'errors': balance_errors,
                'error_count': len(balance_errors),
                'error_rate': len(balance_errors) / len(df),
                'status': 'OK' if len(balance_errors) == 0 else 'ERROR'
            }
        
        # ë‚ ì§œ ìˆœì„œ ê²€ì‚¬
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            is_sorted = df['date'].is_monotonic_increasing
            
            results['date_order'] = {
                'is_sorted': is_sorted,
                'status': 'OK' if is_sorted else 'WARNING',
                'message': 'Dates are in chronological order' if is_sorted else 'Dates are not in chronological order'
            }
        
        return results
    
    def validate_format(self, df: pd.DataFrame) -> Dict:
        """ë°ì´í„° í˜•ì‹ ê²€ì¦"""
        results = {}
        
        # ê¸ˆì•¡ í˜•ì‹ ê²€ì‚¬
        amount_columns = ['amount', 'withdrawal', 'deposit', 'balance']
        for col in amount_columns:
            if col in df.columns:
                non_numeric = df[~df[col].apply(lambda x: pd.isna(x) or isinstance(x, (int, float)))]
                
                results[f'{col}_format'] = {
                    'valid_count': len(df) - len(non_numeric),
                    'invalid_count': len(non_numeric),
                    'invalid_indices': non_numeric.index.tolist(),
                    'status': 'OK' if len(non_numeric) == 0 else 'ERROR'
                }
        
        # ë‚ ì§œ í˜•ì‹ ê²€ì‚¬
        if 'date' in df.columns:
            date_errors = []
            for idx, date_val in df['date'].items():
                try:
                    pd.to_datetime(date_val)
                except:
                    date_errors.append(idx)
            
            results['date_format'] = {
                'valid_count': len(df) - len(date_errors),
                'invalid_count': len(date_errors),
                'invalid_indices': date_errors,
                'status': 'OK' if len(date_errors) == 0 else 'ERROR'
            }
        
        return results
    
    def validate_business_rules(self, df: pd.DataFrame) -> Dict:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦"""
        results = {}
        
        # ìŒìˆ˜ ì”ì•¡ ê²€ì‚¬
        if 'balance' in df.columns:
            negative_balance = df[df['balance'] < 0]
            results['negative_balance'] = {
                'count': len(negative_balance),
                'indices': negative_balance.index.tolist(),
                'status': 'WARNING' if len(negative_balance) > 0 else 'OK'
            }
        
        # ë¹„ì •ìƒì ì¸ ê±°ë˜ ê¸ˆì•¡ ê²€ì‚¬
        if 'amount' in df.columns:
            q99 = df['amount'].quantile(0.99)
            q01 = df['amount'].quantile(0.01)
            outliers = df[(df['amount'] > q99 * 10) | (df['amount'] < q01 / 10)]
            
            results['amount_outliers'] = {
                'count': len(outliers),
                'indices': outliers.index.tolist(),
                'threshold_upper': q99 * 10,
                'threshold_lower': q01 / 10,
                'status': 'WARNING' if len(outliers) > 0 else 'OK'
            }
        
        # ì¤‘ë³µ ê±°ë˜ ê²€ì‚¬
        duplicate_columns = ['date', 'time', 'amount', 'description']
        available_columns = [col for col in duplicate_columns if col in df.columns]
        
        if available_columns:
            duplicates = df[df.duplicated(subset=available_columns, keep=False)]
            results['duplicate_transactions'] = {
                'count': len(duplicates),
                'indices': duplicates.index.tolist(),
                'status': 'WARNING' if len(duplicates) > 0 else 'OK'
            }
        
        return results
    
    def generate_quality_report(self, df: pd.DataFrame) -> str:
        """ì¢…í•© í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        report = []
        report.append("# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë³´ê³ ì„œ\n")
        report.append(f"**ê²€ì¦ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append(f"**ì´ ë ˆì½”ë“œ ìˆ˜**: {len(df)}\n")
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
        required_columns = ['date', 'description', 'amount', 'balance']
        completeness = self.validate_completeness(df, required_columns)
        
        report.append("## 1. ë°ì´í„° ì™„ì „ì„±\n")
        for col, result in completeness.items():
            status_emoji = "âœ…" if result.get('status') == 'OK' else "âš ï¸" if result.get('status') == 'WARNING' else "âŒ"
            report.append(f"- {col}: {status_emoji} {result.get('completeness', 0):.1%} ì™„ì„±ë„")
        
        # ì¼ê´€ì„± ê²€ì¦
        consistency = self.validate_consistency(df)
        
        report.append("\n## 2. ë°ì´í„° ì¼ê´€ì„±\n")
        for check, result in consistency.items():
            status_emoji = "âœ…" if result['status'] == 'OK' else "âš ï¸" if result['status'] == 'WARNING' else "âŒ"
            report.append(f"- {check}: {status_emoji} {result.get('message', '')}")
        
        # í˜•ì‹ ê²€ì¦
        format_check = self.validate_format(df)
        
        report.append("\n## 3. ë°ì´í„° í˜•ì‹\n")
        for check, result in format_check.items():
            status_emoji = "âœ…" if result['status'] == 'OK' else "âŒ"
            report.append(f"- {check}: {status_emoji} {result['valid_count']}/{result['valid_count'] + result['invalid_count']} ìœ íš¨")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
        business_rules = self.validate_business_rules(df)
        
        report.append("\n## 4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™\n")
        for rule, result in business_rules.items():
            status_emoji = "âœ…" if result['status'] == 'OK' else "âš ï¸"
            report.append(f"- {rule}: {status_emoji} {result['count']}ê±´ ê°ì§€")
        
        return '\n'.join(report)

# ì‚¬ìš© ì˜ˆì‹œ
validator = DataQualityValidator()
quality_report = validator.generate_quality_report(df)
print(quality_report)

# ê²€ì¦ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
validation_results = {
    'completeness': validator.validate_completeness(df, ['date', 'amount', 'balance']),
    'consistency': validator.validate_consistency(df),
    'format': validator.validate_format(df),
    'business_rules': validator.validate_business_rules(df)
}

with open('validation_results.json', 'w', encoding='utf-8') as f:
    json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
```

---

## 4. ì‹¤ìŠµ ì˜ˆì œ

### 4.1 ë‹¨ê³„ë³„ ë°ì´í„° ë³€í™˜ ì‹¤ìŠµ

```python
# Step 1: ìƒ˜í”Œ ë°ì´í„° ìƒì„±
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random

def generate_sample_transactions(n_days=30):
    """ìƒ˜í”Œ ê±°ë˜ ë°ì´í„° ìƒì„±"""
    
    merchants = {
        'ìŠ¤íƒ€ë²…ìŠ¤': ('ì¹´í˜', 3000, 8000),
        'ì´ë§ˆíŠ¸': ('ë§ˆíŠ¸', 20000, 150000),
        'GS25': ('í¸ì˜ì ', 1000, 20000),
        'ë§¥ë„ë‚ ë“œ': ('íŒ¨ìŠ¤íŠ¸í‘¸ë“œ', 5000, 15000),
        'ì„œë¸Œì›¨ì´': ('íŒ¨ìŠ¤íŠ¸í‘¸ë“œ', 6000, 12000),
        'ì˜¬ë¦¬ë¸Œì˜': ('í™”ì¥í’ˆ', 10000, 50000),
        'êµë³´ë¬¸ê³ ': ('ì„œì ', 10000, 80000),
        'CGV': ('ì˜í™”ê´€', 10000, 30000),
        'ì¿ íŒ¡': ('ì˜¨ë¼ì¸ì‡¼í•‘', 10000, 200000),
        'ë„¤ì´ë²„í˜ì´': ('ì˜¨ë¼ì¸ê²°ì œ', 5000, 100000)
    }
    
    transactions = []
    current_balance = 5000000  # ì´ˆê¸° ì”ì•¡ 500ë§Œì›
    current_date = datetime.now() - timedelta(days=n_days)
    
    # ì›”ê¸‰ ì¶”ê°€
    transactions.append({
        'ê±°ë˜ì¼ì': current_date.strftime('%Y-%m-%d'),
        'ê±°ë˜ì‹œê°„': '09:00:00',
        'ì ìš”': 'ê¸‰ì—¬',
        'ì¶œê¸ˆì•¡': '',
        'ì…ê¸ˆì•¡': 3500000,
        'ì”ì•¡': current_balance + 3500000,
        'ê±°ë˜ì ': 'ìë™ì´ì²´'
    })
    current_balance += 3500000
    
    for day in range(n_days):
        current_date += timedelta(days=1)
        n_transactions = random.randint(0, 5)
        
        for _ in range(n_transactions):
            merchant = random.choice(list(merchants.keys()))
            category, min_amount, max_amount = merchants[merchant]
            amount = random.randint(min_amount, max_amount)
            
            # ì¶œê¸ˆ ê±°ë˜
            current_balance -= amount
            
            transaction = {
                'ê±°ë˜ì¼ì': current_date.strftime('%Y-%m-%d'),
                'ê±°ë˜ì‹œê°„': f"{random.randint(8, 22):02d}:{random.randint(0, 59):02d}:{random.randint(0, 59):02d}",
                'ì ìš”': merchant,
                'ì¶œê¸ˆì•¡': amount,
                'ì…ê¸ˆì•¡': '',
                'ì”ì•¡': current_balance,
                'ê±°ë˜ì ': random.choice(['ì˜¨ë¼ì¸', 'ì²´í¬ì¹´ë“œ', 'ì‹ ìš©ì¹´ë“œ'])
            }
            transactions.append(transaction)
    
    return pd.DataFrame(transactions)

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì €ì¥
sample_df = generate_sample_transactions(30)
sample_df.to_csv('sample_transactions.csv', index=False, encoding='utf-8-sig')
print(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_df)}ê±´ì˜ ê±°ë˜")
```

### 4.2 í†µí•© ë³€í™˜ íŒŒì´í”„ë¼ì¸

```python
class FinancialDataPipeline:
    """ê¸ˆìœµ ë°ì´í„° ë³€í™˜ í†µí•© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, input_file, output_format='json'):
        self.input_file = input_file
        self.output_format = output_format
        self.df = None
        self.processed_data = None
    
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        self.df = pd.read_csv(self.input_file, encoding='utf-8-sig')
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê±´")
        return self
    
    def clean_data(self):
        """ë°ì´í„° ì •ì œ"""
        # ë¹ˆ ê°’ ì²˜ë¦¬
        self.df['ì¶œê¸ˆì•¡'] = pd.to_numeric(self.df['ì¶œê¸ˆì•¡'], errors='coerce').fillna(0)
        self.df['ì…ê¸ˆì•¡'] = pd.to_numeric(self.df['ì…ê¸ˆì•¡'], errors='coerce').fillna(0)
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        self.df['ê±°ë˜ì¼ì'] = pd.to_datetime(self.df['ê±°ë˜ì¼ì'])
        
        print("âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ")
        return self
    
    def enrich_data(self):
        """ë°ì´í„° ë³´ê°•"""
        # ê±°ë˜ ìœ í˜• ì¶”ê°€
        self.df['ê±°ë˜ìœ í˜•'] = self.df.apply(
            lambda x: 'ì…ê¸ˆ' if x['ì…ê¸ˆì•¡'] > 0 else 'ì¶œê¸ˆ', axis=1
        )
        
        # ê±°ë˜ ê¸ˆì•¡ í†µí•©
        self.df['ê¸ˆì•¡'] = self.df.apply(
            lambda x: x['ì…ê¸ˆì•¡'] if x['ì…ê¸ˆì•¡'] > 0 else x['ì¶œê¸ˆì•¡'], axis=1
        )
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        def categorize(description):
            categories = {
                'ì¹´í˜': ['ìŠ¤íƒ€ë²…ìŠ¤', 'ì»¤í”¼', 'ì¹´í˜'],
                'ë§ˆíŠ¸': ['ì´ë§ˆíŠ¸', 'í™ˆí”ŒëŸ¬ìŠ¤', 'ë¡¯ë°ë§ˆíŠ¸'],
                'í¸ì˜ì ': ['GS25', 'CU', 'ì„¸ë¸ì¼ë ˆë¸'],
                'ì‹ì‚¬': ['ë§¥ë„ë‚ ë“œ', 'ì„œë¸Œì›¨ì´', 'ë²„ê±°í‚¹'],
                'ì‡¼í•‘': ['ì¿ íŒ¡', 'ë„¤ì´ë²„', 'ì˜¬ë¦¬ë¸Œì˜'],
                'ê¸‰ì—¬': ['ê¸‰ì—¬', 'ì›”ê¸‰', 'ìƒì—¬'],
                'ê¸°íƒ€': []
            }
            
            for category, keywords in categories.items():
                if any(keyword in description for keyword in keywords):
                    return category
            return 'ê¸°íƒ€'
        
        self.df['ì¹´í…Œê³ ë¦¬'] = self.df['ì ìš”'].apply(categorize)
        
        print("âœ… ë°ì´í„° ë³´ê°• ì™„ë£Œ")
        return self
    
    def validate_data(self):
        """ë°ì´í„° ê²€ì¦"""
        validator = DataQualityValidator()
        report = validator.generate_quality_report(self.df)
        
        print("âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        print(report)
        return self
    
    def convert_to_json(self):
        """JSON ë³€í™˜"""
        result = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'total_records': len(self.df),
                'period': {
                    'start': self.df['ê±°ë˜ì¼ì'].min().isoformat(),
                    'end': self.df['ê±°ë˜ì¼ì'].max().isoformat()
                }
            },
            'summary': {
                'total_deposits': float(self.df[self.df['ê±°ë˜ìœ í˜•'] == 'ì…ê¸ˆ']['ê¸ˆì•¡'].sum()),
                'total_withdrawals': float(self.df[self.df['ê±°ë˜ìœ í˜•'] == 'ì¶œê¸ˆ']['ê¸ˆì•¡'].sum()),
                'category_breakdown': self.df[self.df['ê±°ë˜ìœ í˜•'] == 'ì¶œê¸ˆ'].groupby('ì¹´í…Œê³ ë¦¬')['ê¸ˆì•¡'].sum().to_dict()
            },
            'transactions': []
        }
        
        for _, row in self.df.iterrows():
            transaction = {
                'date': row['ê±°ë˜ì¼ì'].isoformat(),
                'time': row['ê±°ë˜ì‹œê°„'],
                'description': row['ì ìš”'],
                'type': row['ê±°ë˜ìœ í˜•'],
                'amount': float(row['ê¸ˆì•¡']),
                'balance': float(row['ì”ì•¡']),
                'category': row['ì¹´í…Œê³ ë¦¬'],
                'channel': row['ê±°ë˜ì ']
            }
            result['transactions'].append(transaction)
        
        self.processed_data = result
        print("âœ… JSON ë³€í™˜ ì™„ë£Œ")
        return self
    
    def convert_to_markdown(self):
        """Markdown ë³€í™˜"""
        generator = MarkdownReportGenerator(self.df)
        self.processed_data = generator.generate_financial_report()
        
        print("âœ… Markdown ë³€í™˜ ì™„ë£Œ")
        return self
    
    def save_output(self, output_file=None):
        """ê²°ê³¼ ì €ì¥"""
        if output_file is None:
            extension = 'json' if self.output_format == 'json' else 'md'
            output_file = f"processed_data.{extension}"
        
        if self.output_format == 'json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_data, f, ensure_ascii=False, indent=2, default=str)
        else:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(self.processed_data)
        
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        return self
    
    def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.load_data()
        self.clean_data()
        self.enrich_data()
        self.validate_data()
        
        if self.output_format == 'json':
            self.convert_to_json()
        else:
            self.convert_to_markdown()
        
        self.save_output()
        
        print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        return self

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline = FinancialDataPipeline('sample_transactions.csv', output_format='json')
pipeline.run()

# Markdown ë²„ì „ë„ ìƒì„±
pipeline_md = FinancialDataPipeline('sample_transactions.csv', output_format='markdown')
pipeline_md.run()
```

---

## í•µì‹¬ ìš”ì•½

1. **ë°ì´í„° ë³€í™˜ì˜ ëª©ì **: AI í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ ë° ë§¥ë½ ì •ë³´ ë³´ì¡´
2. **ì£¼ìš” ë³€í™˜ ë„êµ¬**: pandas, json, markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
3. **í’ˆì§ˆ ê´€ë¦¬**: ì™„ì „ì„±, ì¼ê´€ì„±, í˜•ì‹, ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
4. **ì‹¤ë¬´ ì ìš©**: íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì„ í†µí•œ ìë™í™”

---

## ë‹¤ìŒ ì°¨ì‹œ ì˜ˆê³ 

**2-3. .md, .json í˜•íƒœ ë³€í™˜ ì‹¤ìŠµ**
- RAG ì‹œìŠ¤í…œê³¼ì˜ ì—°ê³„
- ë²¡í„° DB ì €ì¥ ë° ê²€ìƒ‰
- ì‹¤ì œ ê¸ˆìœµ ì±—ë´‡ êµ¬í˜„